# -*- coding: utf-8 -*-
"""JAX4MAX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pzKd5siw7a5BqAv1AgAJJ_4dbYe2WW9r
"""

!pip install equinox
!pip install kaggle
import kaggle
import jax
import equinox
import optax
import matplotlib
import os
import PIL.Image
import PIL.ImageEnhance
import math
import random
import numpy as np
from typing import Sequence

if not os.path.exists("/content/kaggle.json"):
  from google.colab import files
  uploaded = files.upload()
  !mkdir -p ~/.kaggle
  !cp kaggle.json ~/.kaggle/
  !chmod 600 ~/.kaggle/kaggle.json
kaggle.api.authenticate()

if os.path.exists("/content/sample_data"):
  !rm -rf sample_data

"""Download and inspect a Kaggle wound dataset, checking image sizes, label distribution, and metadata"""

if not os.path.exists("/content/data/Wound_dataset copy"):
  kaggle.api.dataset_download_files("ibrahimfateen/wound-classification", path="./data", unzip=True)

label_counts = {}
size_counts = {}
image_info = []
for label in os.listdir("/content/data/Wound_dataset copy"):
  label_path = os.path.join("/content/data/Wound_dataset copy", label)
  if not os.path.isdir(label_path):
    continue
  for fname in os.listdir(label_path):
    if not fname.lower().endswith((".jpg")):
      continue
    fpath = os.path.join(label_path, fname)
    try:
      with PIL.Image.open(fpath) as img:
          w, h = img.size
    except Exception as e:
      print("Error reading", fpath, e)
      continue
    if label not in label_counts:
      label_counts[label] = 0
    label_counts[label] += 1
    key = (w, h)
    if key not in size_counts:
      size_counts[key] = 0
    size_counts[key] += 1
    image_info.append({
      "path": fpath,
      "label": label,
      "width": w,
      "height": h,
    })
print("Label counts:", label_counts)
print("Size counts (first few):", list(size_counts.items())[:10])

"""From data name, seems like there are already some augmentation (mirroring of data)"""

os.makedirs("/content/data/dataset", exist_ok=True)
index = 1
for label in os.listdir("/content/data/Wound_dataset copy"):
  label_path = os.path.join("/content/data/Wound_dataset copy", label)
  if not os.path.isdir(label_path):
    continue
  for fname in os.listdir(label_path):
    if not fname.lower().endswith(".jpg"):
      continue
    src_path = os.path.join(label_path, fname)
    safe_label = label.replace(" ", "_")
    new_name = f"{index:06d}_{safe_label}.jpg"
    dst_path = os.path.join("/content/data/dataset", new_name)
    with open(src_path, "rb") as fsrc:
      data = fsrc.read()
    with open(dst_path, "wb") as fdst:
      fdst.write(data)
    index += 1

"""Image resizing before image augmentation"""

unique_sizes = set()
for fname in os.listdir("/content/data/dataset"):
  if not fname.lower().endswith(".jpg"):
    continue
  fpath = os.path.join("/content/data/dataset", fname)
  try:
    with PIL.Image.open(fpath) as img:
        w, h = img.size
  except Exception as e:
    print("Error reading", fpath, e)
    continue
  unique_sizes.add((w, h))
print("Number of unique sizes:", len(unique_sizes))
print("Unique sizes:")
for w, h in sorted(unique_sizes):
  print(f"{w}x{h}")

"""Way too many different sizes..."""

def preprocess_folder(dst_dir,target_size=(224, 224),mode="resize",extensions=(".jpg"),):
  """
  "resize": simple resize, may distort aspect ratio
  "resize_pad": keep aspect ratio, pad with black
  "center_crop": center-crop then resize to target_size
  """
  os.makedirs(dst_dir, exist_ok=True)
  for fname in os.listdir("./data/dataset"):
    if not fname.lower().endswith(extensions):
      continue
    src_path = os.path.join("./data/dataset", fname)
    dst_path = os.path.join(dst_dir, fname)
    try:
      img = PIL.Image.open(src_path).convert("RGB")
    except Exception as e:
      print("Error:", src_path, e)
      continue
    if mode == "resize":
      img = img.resize(target_size, PIL.Image.LANCZOS)
    elif mode == "resize_pad":
      img.thumbnail(target_size, PIL.Image.LANCZOS)
      new_img = PIL.Image.new("RGB", target_size, (0, 0, 0))
      w, h = img.size
      left = (target_size[0] - w) // 2
      top = (target_size[1] - h) // 2
      new_img.paste(img, (left, top))
      img = new_img
    elif mode == "center_crop":
      w, h = img.size
      min_side = min(w, h)
      left = (w - min_side) // 2
      top = (h - min_side) // 2
      right = left + min_side
      bottom = top + min_side
      img = img.crop((left, top, right, bottom))
      img = img.resize(target_size, PIL.Image.LANCZOS)
    else:
      raise ValueError(f"Unknown mode: {mode}")
    img.save(dst_path, quality=95)

preprocess_folder("./data/2baugmented/dataset_low", (64,64))
preprocess_folder("./data/2baugmented/dataset_mid", (128,128))
preprocess_folder("./data/2baugmented/dataset_hig", (256,256))

def random_rotate(img, degrees=20):
  angle = random.uniform(-degrees, degrees)
  return img.rotate(angle, resample=PIL.Image.BILINEAR)

def center_crop_resize(img, target_size):
  width, height = img.size
  min_side = min(width, height)
  left = (width - min_side) // 2
  top = (height - min_side) // 2
  right = left + min_side
  bottom = top + min_side
  img_cropped = img.crop((left, top, right, bottom))
  return img_cropped.resize(target_size, PIL.Image.BILINEAR)

def random_resized_crop(img, target_size, scale=(0.9, 1.0), ratio=(0.9, 1.1)):
  width, height = img.size
  area = width * height
  for _ in range(10):
    target_area = random.uniform(scale[0], scale[1]) * area
    log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
    aspect = math.exp(random.uniform(*log_ratio))
    w = int(round(math.sqrt(target_area * aspect)))
    h = int(round(math.sqrt(target_area / aspect)))
    if 0 < w <= width and 0 < h <= height:
      left = random.randint(0, width - w)
      top = random.randint(0, height - h)
      img_cropped = img.crop((left, top, left + w, top + h))
      return img_cropped.resize(target_size, PIL.Image.BILINEAR)
  return center_crop_resize(img, target_size)

def random_brightness(img, max_delta=0.2):
  factor = 1.0 + random.uniform(-max_delta, max_delta)
  enhancer = PIL.ImageEnhance.Brightness(img)
  return enhancer.enhance(factor)

def random_contrast(img, max_delta=0.2):
  factor = 1.0 + random.uniform(-max_delta, max_delta)
  enhancer = PIL.ImageEnhance.Contrast(img)
  return enhancer.enhance(factor)

def augment_image_pil(img, target_size, degrees=20):
  img = random_rotate(img, degrees=degrees)
  img = random_resized_crop(
    img,
    target_size=target_size,
    scale=(0.9, 1.0),
    ratio=(0.9, 1.1),
  )
  img = random_brightness(img, max_delta=0.2)
  img = random_contrast(img, max_delta=0.2)
  return img

def augment_folder(src_dir, dst_dir, target_size, num_aug_per_image=1):
  os.makedirs(dst_dir, exist_ok=True)
  for fname in os.listdir(src_dir):
    if not fname.lower().endswith(".jpg"):
      continue
    src_path = os.path.join(src_dir, fname)
    try:
      img = PIL.Image.open(src_path).convert("RGB")
    except Exception as e:
      print("Error:", src_path, e)
      continue
    base_name, ext = os.path.splitext(fname)
    for i in range(num_aug_per_image):
      aug_img = augment_image_pil(img, target_size=target_size, degrees=20)
      new_name = f"{base_name}_aug{i:02d}{ext}"
      dst_path = os.path.join(dst_dir, new_name)
      aug_img.save(dst_path, quality=95)

augment_folder(src_dir="/content/data/2baugmented/dataset_low",dst_dir="/content/data/augmented/dataset_64_low",target_size=(64, 64),num_aug_per_image=2)
augment_folder(src_dir="/content/data/2baugmented/dataset_mid",dst_dir="/content/data/augmented/dataset_128_mid",target_size=(128, 128),num_aug_per_image=2)
augment_folder(src_dir="/content/data/2baugmented/dataset_hig",dst_dir="/content/data/augmented/dataset_256_hig",target_size=(256, 256),num_aug_per_image=2)

"""Newest dataset to be used is /data/augmented which has been preprocessed

Next is creating a CNN using Jax/Equinox
This one has really good accuracy https://arxiv.org/html/2408.11064v1
"""

class ConvolutionalBlock(equinox.Module):
  conv: equinox.nn.Conv2d
  norm: equinox.nn.BatchNorm
  act: equinox.nn.Lambda

  def __init__(self, in_ch, out_ch, *, key):
    k1, k2 = jax.random.split(key, 2)
    self.conv = equinox.nn.Conv2d(
      in_channels=in_ch,
      out_channels=out_ch,
      kernel_size=3,
      padding=1,
      key=k1,
    )
    self.norm = equinox.nn.BatchNorm(out_ch, axis_name="batch", momentum=0.9, eps=1e-5)
    self.act = equinox.nn.Lambda(jax.nn.relu)

  def __call__(self, x, *, key=None):
    x = self.conv(x)
    x = self.norm(x)
    x = self.act(x)
    return x

class Baseline(equinox.Module):
  conv_blocks: Sequence[ConvolutionalBlock]
  fc1: equinox.nn.Linear
  fc2: equinox.nn.Linear
  fc3: equinox.nn.Linear
  dropout: equinox.nn.Dropout

  def __init__(self, image_size, in_ch, num_classes, *, key):
    k_conv, k_fc1, k_fc2, k_fc3, k_do = jax.random.split(key, 5)

    channels = [in_ch, 32, 64, 128, 256]
    blocks = []
    k = k_conv
    for i in range(4):
      k, subk = jax.random.split(k)
      blocks.append(ConvolutionalBlock(channels[i], channels[i+1], key=subk))
    self.conv_blocks = tuple(blocks)
    spatial = image_size // 16
    flattened_dim = channels[-1] * spatial * spatial

    self.fc1 = equinox.nn.Linear(flattened_dim, 256, key=k_fc1)
    self.fc2 = equinox.nn.Linear(256, 128, key=k_fc2)
    self.fc3 = equinox.nn.Linear(128, num_classes, key=k_fc3)
    self.dropout = equinox.nn.Dropout(p=0.5, key=k_do)

  def __call__(self, x, *, key=None, train=True):
    x = jax.numpy.transpose(x, (0, 3, 1, 2))

    for block in self.conv_blocks:
      x = block(x)
      x = jax.lax.reduce_window(
        x,
        -jax.numpy.inf,
        jax.lax.max,
        window_dimensions=(1, 1, 2, 2),
        window_strides=(1, 1, 2, 2),
        padding="VALID",
      )

    x = x.reshape(x.shape[0], -1)

    if key is None:
      key = jax.random.PRNGKey(0)
    k1, k2 = jax.random.split(key, 2)

    x = self.fc1(x)
    x = jax.nn.relu(x)
    x = self.dropout(x, key=k1, inference=not train)

    x = self.fc2(x)
    x = jax.nn.relu(x)
    x = self.dropout(x, key=k2, inference=not train)

    logits = self.fc3(x)
    return logits

def cross_entropy_loss(logits, labels):
  one_hot = jax.nn.one_hot(labels, logits.shape[-1])
  loss = optax.softmax_cross_entropy(logits, one_hot).mean()
  return loss

def compute_accuracy(logits, labels):
  preds = logits.argmax(axis=-1)
  return (preds == labels).mean()

@equinox.filter_jit
def train_step(model, opt_state, x, y, key):
  def loss_fn(m):
    logits = m(x, key=key, train=True)
    loss = cross_entropy_loss(logits, y)
    return loss, logits

  (loss, logits), grads = equinox.filter_value_and_grad(loss_fn, has_aux=True)(model)
  updates, opt_state = optimizer.update(grads, opt_state, model)
  model = equinox.apply_updates(model, updates)
  acc = compute_accuracy(logits, y)
  return model, opt_state, loss, acc


@equinox.filter_jit
def eval_step(model, x, y):
  logits = model(x, key=None, train=False)
  loss = cross_entropy_loss(logits, y)
  acc = compute_accuracy(logits, y)
  return loss, acc

learning_rate = 1e-3
weight_decay = 1e-4
optimizer = optax.adamw(learning_rate=learning_rate, weight_decay=weight_decay)

key = jax.random.PRNGKey(42)
image_size = 64
num_classes = 9

model = Baseline(
  image_size=image_size,
  in_ch=3,
  num_classes=num_classes,
  key=key,
)

opt_state = optimizer.init(equinox.filter(model, equinox.is_inexact_array))

def parse_label_from_name(fname):
  base, _ = os.path.splitext(fname)
  parts = base.split("_", 1)
  if len(parts) != 2:
    raise ValueError(f"Unexpected filename format: {fname}")
  label_part = parts[1]
  if "_aug" in label_part:
    label_part = label_part.split("_aug", 1)[0]
  return label_part


def build_index_flat(root_dir, val_ratio=0.2, seed=0):
  all_files = [f for f in os.listdir(root_dir) if f.lower().endswith(".jpg")]
  labels = [parse_label_from_name(f) for f in all_files]
  class_names = sorted(set(labels))
  label2idx = {name: i for i, name in enumerate(class_names)}

  samples = []
  for fname, lbl in zip(all_files, labels):
    path = os.path.join(root_dir, fname)
    samples.append((path, label2idx[lbl]))

  random.Random(seed).shuffle(samples)
  n_total = len(samples)
  n_val = int(n_total * val_ratio)
  val_samples = samples[:n_val]
  train_samples = samples[n_val:]
  return train_samples, val_samples, label2idx

data_dir = "/content/data/augmented/dataset_64_low"

train_samples, val_samples, label2idx = build_index_flat(data_dir, val_ratio=0.2, seed=42)
num_classes = len(label2idx)
print("classes:", label2idx)
print("train samples:", len(train_samples))
print("val samples:", len(val_samples))

for epoch in range(1, 11):
  train_losses = []
  train_accs = []

  for x_batch_np, y_batch_np in train_loader:  # you define this
    x_batch = jax.numpy.array(x_batch_np, dtype=jax.numpy.float32)
    y_batch = jax.numpy.array(y_batch_np, dtype=jax.numpy.int32)
    key, subkey = jax.random.split(key)
    model, opt_state, loss, acc = train_step(model, opt_state, x_batch, y_batch, subkey)
    train_losses.append(loss)
    train_accs.append(acc)

  train_loss = float(jax.numpy.mean(jax.numpy.array(train_losses)))
  train_acc = float(jax.numpy.mean(jax.numpy.array(train_accs)))

  val_losses = []
  val_accs = []
  for x_batch_np, y_batch_np in val_loader:
    x_batch = jax.numpy.array(x_batch_np, dtype=jax.numpy.float32)
    y_batch = jax.numpy.array(y_batch_np, dtype=jax.numpy.int32)
    loss, acc = eval_step(model, x_batch, y_batch)
    val_losses.append(loss)
    val_accs.append(acc)

  val_loss = float(jax.numpy.mean(jax.numpy.array(val_losses)))
  val_acc = float(jax.numpy.mean(jax.numpy.array(val_accs)))

  print(f"epoch {epoch} train loss {train_loss} acc {train_acc} val loss {val_loss} acc {val_acc}")