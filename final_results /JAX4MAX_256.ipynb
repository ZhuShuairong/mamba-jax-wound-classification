{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPCyb2SP_eJk"
      },
      "source": [
        "# import libraries and downloading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q9wyBuVK0uS-",
        "outputId": "3652b963-aec4-4262-feb7-0fe65191ffd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: equinox in ./jax_env/lib/python3.12/site-packages (0.13.2)\n",
            "Requirement already satisfied: jax!=0.7.0,!=0.7.1,>=0.4.38 in ./jax_env/lib/python3.12/site-packages (from equinox) (0.8.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.20 in ./jax_env/lib/python3.12/site-packages (from equinox) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in ./jax_env/lib/python3.12/site-packages (from equinox) (4.15.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.0 in ./jax_env/lib/python3.12/site-packages (from equinox) (0.1.7)\n",
            "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in ./jax_env/lib/python3.12/site-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (0.8.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in ./jax_env/lib/python3.12/site-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in ./jax_env/lib/python3.12/site-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (2.3.5)\n",
            "Requirement already satisfied: opt_einsum in ./jax_env/lib/python3.12/site-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in ./jax_env/lib/python3.12/site-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (1.16.3)\n",
            "Requirement already satisfied: kaggle in ./jax_env/lib/python3.12/site-packages (1.8.2)\n",
            "Requirement already satisfied: black>=24.10.0 in ./jax_env/lib/python3.12/site-packages (from kaggle) (25.11.0)\n",
            "Requirement already satisfied: bleach in ./jax_env/lib/python3.12/site-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: kagglesdk in ./jax_env/lib/python3.12/site-packages (from kaggle) (0.1.13)\n",
            "Requirement already satisfied: mypy>=1.15.0 in ./jax_env/lib/python3.12/site-packages (from kaggle) (1.19.0)\n",
            "Requirement already satisfied: protobuf in ./jax_env/lib/python3.12/site-packages (from kaggle) (6.33.1)\n",
            "Requirement already satisfied: python-dateutil in ./jax_env/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in ./jax_env/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in ./jax_env/lib/python3.12/site-packages (from kaggle) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in ./jax_env/lib/python3.12/site-packages (from kaggle) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in ./jax_env/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: tqdm in ./jax_env/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: types-requests in ./jax_env/lib/python3.12/site-packages (from kaggle) (2.32.4.20250913)\n",
            "Requirement already satisfied: types-tqdm in ./jax_env/lib/python3.12/site-packages (from kaggle) (4.67.0.20250809)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in ./jax_env/lib/python3.12/site-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in ./jax_env/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in ./jax_env/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (1.1.0)\n",
            "Requirement already satisfied: packaging>=22.0 in ./jax_env/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (25.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in ./jax_env/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in ./jax_env/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in ./jax_env/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.6.0 in ./jax_env/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (4.15.0)\n",
            "Requirement already satisfied: librt>=0.6.2 in ./jax_env/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (0.6.3)\n",
            "Requirement already satisfied: webencodings in ./jax_env/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in ./jax_env/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./jax_env/lib/python3.12/site-packages (from requests->kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./jax_env/lib/python3.12/site-packages (from requests->kaggle) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./jax_env/lib/python3.12/site-packages (from requests->kaggle) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install equinox\n",
        "!pip install kaggle\n",
        "!pip install -q flax\n",
        "import equinox\n",
        "from flax.training import train_state\n",
        "import flax.linen\n",
        "from flax import serialization\n",
        "import jax\n",
        "import optax\n",
        "import matplotlib\n",
        "import os\n",
        "import PIL.Image\n",
        "import PIL.ImageEnhance\n",
        "import math\n",
        "import random\n",
        "import numpy\n",
        "from typing import Sequence, List, Tuple\n",
        "import torch\n",
        "import time\n",
        "import pickle\n",
        "import gc\n",
        "import json\n",
        "import flax\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE_DIR: C:\\Users\\zshua\\Downloads\\deeplearningproject\\content\n",
            "DATA_DIR: C:\\Users\\zshua\\Downloads\\deeplearningproject\\content/data\n"
          ]
        }
      ],
      "source": [
        "def get_base_dir(preferred=None):\n",
        "  if preferred:\n",
        "    return preferred\n",
        "  env_dir = os.getenv(\"BASE_DIR\")\n",
        "  if env_dir:\n",
        "    return env_dir\n",
        "  if os.path.exists(\"/kaggle/working\"):\n",
        "    return \"/kaggle/working\"\n",
        "  if os.path.exists(\"/content\"):\n",
        "    return \"/content\"\n",
        "  return os.getcwd()\n",
        "BASE_DIR = get_base_dir(preferred=r\"C:\\Users\\zshua\\Downloads\\deeplearningproject\\content\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "KAGGLE_JSON_PATH = os.path.join(BASE_DIR, \"kaggle.json\")\n",
        "def bpath(*parts):\n",
        "  return os.path.join(BASE_DIR, *parts)\n",
        "def data_path(*parts):\n",
        "  return os.path.join(DATA_DIR, *parts)\n",
        "print(f\"BASE_DIR: {BASE_DIR}\")\n",
        "print(f\"DATA_DIR: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcbluncAE5kk"
      },
      "source": [
        "# code will not run if you don't provide kaggle login!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "G76HUW7K_3R0",
        "outputId": "e9f66903-0b18-4869-f7de-1d4062f235de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not copy kaggle.json to config dir: [Errno 2] No such file or directory: 'C:\\\\Users\\\\zshua\\\\Downloads\\\\deeplearningproject\\\\content/kaggle.json'\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "if not os.path.exists(KAGGLE_JSON_PATH):\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "'''\n",
        "os.makedirs(os.path.expanduser(\"~/.config/kaggle\"), exist_ok=True)\n",
        "try:\n",
        "  shutil.copyfile(KAGGLE_JSON_PATH, os.path.expanduser(\"~/.config/kaggle/kaggle.json\"))\n",
        "  try:\n",
        "    os.chmod(os.path.expanduser(\"~/.config/kaggle/kaggle.json\"), 0o600)\n",
        "  except Exception:\n",
        "    pass\n",
        "except Exception as e:\n",
        "  print(\"Could not copy kaggle.json to config dir:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Do-SDjzxJkrv"
      },
      "outputs": [],
      "source": [
        "import kaggle #sometimes doesn't work if we import kaggle a the very start so bear with the ugly placement\n",
        "kaggle.api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PwbBBeSqD4H3"
      },
      "outputs": [],
      "source": [
        "sample_data_path = bpath('sample_data')\n",
        "if os.path.exists(sample_data_path):\n",
        "  import shutil\n",
        "  shutil.rmtree(sample_data_path)\n",
        "#dont mind this, this was for before when we placed the data into a sample_data to quickly manipulate the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUolwrEL6bnH"
      },
      "source": [
        "Download and inspect a Kaggle wound dataset, checking image sizes, label distribution, and metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8InGcnay5DKF",
        "outputId": "81b6a54e-b27d-4c61-80f6-e89bdbdafe48"
      },
      "outputs": [],
      "source": [
        "# Download Wound dataset from Kaggle if not present; support different base paths\n",
        "wound_dataset_dir = data_path('Wound_dataset copy')\n",
        "if not os.path.exists(wound_dataset_dir): # just in case since we might be using run all out of pure laziness a couple of times\n",
        "  kaggle.api.dataset_download_files(\"ibrahimfateen/wound-classification\", path=DATA_DIR, unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVFiClWf7twC",
        "outputId": "a4a8c37d-3bb8-43e5-f482-7d3eff61d63f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base path is: C:\\Users\\zshua\\Downloads\\deeplearningproject\\content\n"
          ]
        }
      ],
      "source": [
        "base_path = BASE_DIR\n",
        "print(f\"Base path is: {base_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urwI4EkrDcxq",
        "outputId": "75b8614c-87b1-41bf-83ab-80413777360f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Normal': 100, 'Cut': 50, 'Laseration': 61, 'Bruises': 121, 'Diabetic Wounds': 231, 'Surgical Wounds': 210, 'Pressure Wounds': 301, 'Burns': 67, 'Abrasions': 82, 'Venous Wounds': 247}\n",
            "[((119, 126), 1), ((135, 113), 1)]\n"
          ]
        }
      ],
      "source": [
        "label_counts = {}\n",
        "size_counts = {}\n",
        "image_info = []\n",
        "root_wound_dir = data_path('Wound_dataset copy')\n",
        "for label in os.listdir(root_wound_dir):\n",
        "  label_path = os.path.join(root_wound_dir, label)\n",
        "  if not os.path.isdir(label_path):\n",
        "    continue\n",
        "  for filename in os.listdir(label_path):\n",
        "    if not filename.lower().endswith((\".jpg\")):\n",
        "      continue\n",
        "    filepath = os.path.join(label_path, filename)\n",
        "    try:\n",
        "      with PIL.Image.open(filepath) as img:\n",
        "          w, h = img.size\n",
        "    except Exception as e:\n",
        "      print(\"Cannot find the file\", filepath, e)\n",
        "      continue\n",
        "    if label not in label_counts:\n",
        "      label_counts[label] = 0\n",
        "    label_counts[label] += 1\n",
        "    key = (w, h)\n",
        "    if key not in size_counts:\n",
        "      size_counts[key] = 0\n",
        "    size_counts[key] += 1\n",
        "    image_info.append({\n",
        "      \"path\": filepath,\n",
        "      \"label\": label,\n",
        "      \"width\": w,\n",
        "      \"height\": h,\n",
        "    })\n",
        "print(label_counts)\n",
        "print(list(size_counts.items())[:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pWCU2YX7iee",
        "outputId": "d34ca67e-1111-43dc-ca35-1755d6ecf06c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1470\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "dataset_path = data_path('Wound_dataset copy')\n",
        "mirrored_files = []\n",
        "total_files = 0\n",
        "for root, directories, files in os.walk(dataset_path):\n",
        "  for file in files:\n",
        "    total_files += 1\n",
        "    if 'mirrored' in file.lower():\n",
        "      mirrored_files.append(os.path.join(root,file))\n",
        "#checking for files that haved mirrored inside of them, since from testing, we got higher accuracy when we removed mirrored files (im not sure why, but usually it's bad for a dataset to have multiple copies of the same data)\n",
        "print(total_files)\n",
        "print(len(mirrored_files))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yACUOJqi9ZSG",
        "outputId": "5fa669ea-e2c8-41f7-be52-864a446707fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "what_deleted = []\n",
        "deleted_count = 0\n",
        "for root, directiories, files in os.walk(dataset_path):\n",
        "  for file in files:\n",
        "    if file.startswith('mirrored'):\n",
        "      file_path = os.path.join(root,file)\n",
        "      os.remove(file_path)\n",
        "      what_deleted.append(file_path)\n",
        "      deleted_count += 1\n",
        "print(deleted_count)\n",
        "print(what_deleted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd39-GAS_eCO",
        "outputId": "b4a74d10-87bb-4685-861d-136e0f41695f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1470\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "mirrored_files = []\n",
        "total_files = 0\n",
        "for root, directories, files in os.walk(dataset_path):\n",
        "  for file in files:\n",
        "    total_files += 1\n",
        "    if 'mirrored' in file.lower():\n",
        "      mirrored_files.append(os.path.join(root,file))\n",
        "#check again\n",
        "print(total_files)\n",
        "print(len(mirrored_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0F_i6Poa-Ht8"
      },
      "outputs": [],
      "source": [
        "os.makedirs(data_path('dataset'), exist_ok=True)\n",
        "#renaming files so it's based on the label and index, and moving them into a dataset folder within data\n",
        "index = 1\n",
        "root_wound_dir = data_path('Wound_dataset copy')\n",
        "for label in os.listdir(root_wound_dir):\n",
        "  label_path = os.path.join(root_wound_dir, label)\n",
        "  if not os.path.isdir(label_path):\n",
        "    continue\n",
        "  for filename in os.listdir(label_path):\n",
        "    if not filename.lower().endswith(\".jpg\"):\n",
        "      continue\n",
        "    source_path = os.path.join(label_path, filename)\n",
        "    safe_label = label.replace(\" \",\"_\")\n",
        "    new_name = f\"{index:06d}_{safe_label}.jpg\"\n",
        "    destination_path = os.path.join(data_path('dataset'), new_name)\n",
        "    with open(source_path,\"rb\") as filesource:\n",
        "      data = filesource.read()\n",
        "    with open(destination_path, \"wb\") as filedestination:\n",
        "      filedestination.write(data)\n",
        "    index +=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xQCvVWHeITki",
        "outputId": "fc5c4dcf-0691-4f02-ec97-7a40d715d1ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "585\n",
            "61 78\n",
            "66 86\n",
            "73 85\n",
            "75 89\n",
            "76 76\n",
            "77 87\n",
            "78 78\n",
            "78 82\n",
            "79 79\n",
            "79 80\n",
            "80 80\n",
            "83 82\n",
            "83 93\n",
            "84 75\n",
            "85 85\n",
            "85 86\n",
            "85 89\n",
            "86 95\n",
            "87 95\n",
            "89 89\n",
            "89 109\n",
            "90 89\n",
            "90 90\n",
            "90 107\n",
            "91 90\n",
            "91 91\n",
            "91 92\n",
            "92 85\n",
            "92 88\n",
            "92 93\n",
            "93 89\n",
            "93 93\n",
            "93 100\n",
            "94 80\n",
            "95 67\n",
            "95 91\n",
            "95 95\n",
            "97 96\n",
            "97 98\n",
            "97 103\n",
            "98 98\n",
            "99 98\n",
            "99 99\n",
            "100 99\n",
            "100 100\n",
            "100 116\n",
            "100 122\n",
            "101 101\n",
            "101 102\n",
            "102 102\n",
            "102 105\n",
            "102 106\n",
            "102 125\n",
            "103 120\n",
            "104 103\n",
            "104 104\n",
            "105 97\n",
            "105 105\n",
            "105 113\n",
            "105 120\n",
            "106 105\n",
            "106 106\n",
            "106 112\n",
            "107 107\n",
            "108 108\n",
            "109 109\n",
            "110 110\n",
            "110 111\n",
            "111 111\n",
            "111 112\n",
            "112 106\n",
            "112 111\n",
            "112 112\n",
            "113 98\n",
            "113 113\n",
            "116 116\n",
            "116 118\n",
            "117 109\n",
            "117 117\n",
            "118 108\n",
            "118 110\n",
            "118 118\n",
            "119 118\n",
            "119 119\n",
            "119 126\n",
            "120 128\n",
            "121 87\n",
            "121 116\n",
            "121 121\n",
            "121 122\n",
            "121 129\n",
            "122 111\n",
            "122 122\n",
            "122 123\n",
            "123 106\n",
            "123 122\n",
            "123 123\n",
            "124 114\n",
            "124 124\n",
            "125 121\n",
            "125 125\n",
            "125 141\n",
            "125 146\n",
            "126 105\n",
            "126 126\n",
            "126 149\n",
            "127 107\n",
            "127 127\n",
            "128 110\n",
            "128 122\n",
            "128 128\n",
            "128 139\n",
            "128 182\n",
            "129 122\n",
            "129 130\n",
            "130 104\n",
            "130 113\n",
            "130 114\n",
            "130 130\n",
            "130 131\n",
            "130 132\n",
            "131 125\n",
            "131 131\n",
            "132 115\n",
            "132 132\n",
            "132 167\n",
            "133 112\n",
            "133 118\n",
            "133 132\n",
            "133 133\n",
            "134 134\n",
            "134 143\n",
            "134 144\n",
            "134 149\n",
            "135 113\n",
            "135 135\n",
            "135 136\n",
            "135 146\n",
            "135 154\n",
            "136 136\n",
            "137 123\n",
            "137 136\n",
            "137 137\n",
            "137 140\n",
            "138 132\n",
            "138 136\n",
            "138 137\n",
            "138 138\n",
            "139 139\n",
            "139 140\n",
            "139 172\n",
            "140 140\n",
            "140 153\n",
            "141 129\n",
            "141 141\n",
            "141 142\n",
            "141 148\n",
            "142 139\n",
            "142 142\n",
            "143 130\n",
            "143 143\n",
            "144 144\n",
            "145 145\n",
            "146 146\n",
            "146 148\n",
            "147 121\n",
            "147 147\n",
            "147 148\n",
            "147 152\n",
            "147 153\n",
            "148 148\n",
            "148 149\n",
            "149 135\n",
            "149 137\n",
            "149 148\n",
            "149 149\n",
            "150 148\n",
            "150 155\n",
            "150 167\n",
            "150 172\n",
            "152 152\n",
            "152 157\n",
            "152 164\n",
            "153 153\n",
            "153 154\n",
            "153 219\n",
            "154 99\n",
            "154 101\n",
            "154 102\n",
            "154 103\n",
            "154 104\n",
            "154 105\n",
            "154 106\n",
            "154 107\n",
            "154 108\n",
            "154 109\n",
            "154 110\n",
            "154 111\n",
            "154 112\n",
            "154 113\n",
            "154 114\n",
            "154 115\n",
            "154 116\n",
            "154 117\n",
            "154 118\n",
            "154 119\n",
            "154 120\n",
            "154 121\n",
            "154 122\n",
            "154 123\n",
            "154 124\n",
            "154 126\n",
            "154 127\n",
            "154 128\n",
            "154 129\n",
            "154 130\n",
            "154 131\n",
            "154 134\n",
            "154 137\n",
            "154 139\n",
            "154 142\n",
            "154 144\n",
            "154 154\n",
            "154 193\n",
            "155 155\n",
            "155 158\n",
            "156 156\n",
            "156 183\n",
            "157 156\n",
            "157 157\n",
            "157 164\n",
            "158 121\n",
            "158 158\n",
            "158 162\n",
            "159 158\n",
            "159 159\n",
            "159 167\n",
            "160 160\n",
            "160 161\n",
            "160 178\n",
            "160 184\n",
            "161 161\n",
            "161 192\n",
            "162 162\n",
            "163 135\n",
            "163 162\n",
            "163 163\n",
            "163 164\n",
            "163 173\n",
            "163 192\n",
            "164 138\n",
            "164 142\n",
            "164 164\n",
            "164 165\n",
            "164 197\n",
            "165 146\n",
            "165 165\n",
            "165 166\n",
            "165 171\n",
            "166 148\n",
            "166 166\n",
            "167 152\n",
            "167 159\n",
            "167 167\n",
            "168 143\n",
            "168 144\n",
            "168 168\n",
            "169 169\n",
            "170 166\n",
            "170 170\n",
            "170 186\n",
            "171 156\n",
            "171 171\n",
            "171 172\n",
            "171 181\n",
            "172 160\n",
            "172 172\n",
            "172 203\n",
            "173 173\n",
            "174 174\n",
            "175 158\n",
            "175 172\n",
            "175 175\n",
            "176 175\n",
            "176 176\n",
            "177 174\n",
            "177 177\n",
            "178 178\n",
            "179 178\n",
            "179 179\n",
            "180 167\n",
            "180 180\n",
            "181 181\n",
            "181 205\n",
            "182 182\n",
            "183 183\n",
            "184 184\n",
            "185 184\n",
            "185 185\n",
            "186 158\n",
            "186 162\n",
            "186 185\n",
            "186 186\n",
            "187 187\n",
            "187 188\n",
            "188 167\n",
            "188 188\n",
            "188 213\n",
            "189 188\n",
            "189 189\n",
            "190 152\n",
            "190 190\n",
            "190 197\n",
            "191 191\n",
            "191 201\n",
            "191 208\n",
            "192 192\n",
            "193 132\n",
            "193 193\n",
            "193 210\n",
            "194 194\n",
            "195 163\n",
            "195 194\n",
            "195 195\n",
            "195 196\n",
            "196 196\n",
            "197 197\n",
            "198 198\n",
            "199 178\n",
            "199 200\n",
            "200 200\n",
            "201 200\n",
            "201 201\n",
            "201 202\n",
            "201 206\n",
            "203 203\n",
            "203 204\n",
            "204 178\n",
            "204 204\n",
            "205 205\n",
            "206 165\n",
            "206 206\n",
            "207 206\n",
            "207 207\n",
            "208 208\n",
            "208 209\n",
            "208 224\n",
            "209 209\n",
            "209 219\n",
            "210 210\n",
            "210 213\n",
            "211 211\n",
            "211 223\n",
            "212 212\n",
            "212 217\n",
            "213 212\n",
            "213 213\n",
            "214 214\n",
            "214 256\n",
            "215 207\n",
            "215 215\n",
            "215 216\n",
            "216 216\n",
            "216 231\n",
            "216 283\n",
            "217 200\n",
            "217 217\n",
            "218 218\n",
            "219 219\n",
            "220 219\n",
            "220 220\n",
            "221 221\n",
            "221 259\n",
            "222 222\n",
            "223 223\n",
            "224 224\n",
            "225 224\n",
            "225 225\n",
            "225 272\n",
            "226 225\n",
            "226 226\n",
            "227 227\n",
            "228 206\n",
            "228 228\n",
            "229 219\n",
            "230 206\n",
            "230 230\n",
            "230 240\n",
            "231 231\n",
            "232 232\n",
            "233 233\n",
            "233 235\n",
            "234 290\n",
            "235 200\n",
            "235 232\n",
            "235 235\n",
            "235 285\n",
            "237 237\n",
            "237 238\n",
            "237 274\n",
            "238 224\n",
            "238 239\n",
            "238 253\n",
            "239 196\n",
            "239 239\n",
            "240 240\n",
            "241 241\n",
            "242 242\n",
            "242 243\n",
            "243 238\n",
            "243 244\n",
            "244 208\n",
            "244 244\n",
            "245 227\n",
            "245 244\n",
            "245 245\n",
            "246 239\n",
            "246 246\n",
            "247 228\n",
            "247 247\n",
            "248 247\n",
            "248 248\n",
            "249 249\n",
            "249 271\n",
            "250 250\n",
            "251 212\n",
            "251 250\n",
            "251 252\n",
            "252 252\n",
            "252 274\n",
            "253 252\n",
            "253 269\n",
            "254 254\n",
            "255 240\n",
            "255 254\n",
            "255 255\n",
            "257 235\n",
            "257 272\n",
            "258 258\n",
            "258 273\n",
            "259 259\n",
            "259 273\n",
            "260 302\n",
            "261 261\n",
            "262 240\n",
            "263 262\n",
            "263 263\n",
            "263 264\n",
            "265 265\n",
            "266 266\n",
            "267 253\n",
            "267 261\n",
            "267 267\n",
            "267 268\n",
            "268 268\n",
            "269 269\n",
            "270 290\n",
            "270 292\n",
            "270 321\n",
            "270 324\n",
            "270 336\n",
            "270 374\n",
            "270 457\n",
            "270 476\n",
            "271 271\n",
            "272 272\n",
            "272 294\n",
            "273 273\n",
            "273 327\n",
            "277 277\n",
            "278 240\n",
            "278 277\n",
            "278 278\n",
            "279 279\n",
            "279 295\n",
            "280 280\n",
            "281 281\n",
            "282 297\n",
            "283 283\n",
            "283 303\n",
            "283 332\n",
            "284 329\n",
            "286 286\n",
            "287 273\n",
            "288 288\n",
            "288 332\n",
            "290 240\n",
            "290 294\n",
            "291 306\n",
            "292 250\n",
            "292 292\n",
            "293 302\n",
            "293 327\n",
            "294 324\n",
            "295 313\n",
            "299 299\n",
            "299 337\n",
            "300 240\n",
            "300 300\n",
            "300 312\n",
            "303 305\n",
            "303 332\n",
            "304 383\n",
            "306 306\n",
            "306 346\n",
            "307 308\n",
            "309 308\n",
            "310 304\n",
            "311 315\n",
            "311 326\n",
            "312 312\n",
            "312 327\n",
            "314 240\n",
            "315 325\n",
            "315 327\n",
            "317 322\n",
            "317 336\n",
            "317 338\n",
            "318 240\n",
            "318 315\n",
            "318 316\n",
            "318 332\n",
            "318 342\n",
            "318 376\n",
            "320 226\n",
            "320 233\n",
            "320 240\n",
            "320 320\n",
            "320 329\n",
            "320 332\n",
            "320 333\n",
            "320 337\n",
            "320 338\n",
            "320 339\n",
            "320 344\n",
            "320 347\n",
            "320 350\n",
            "320 361\n",
            "320 366\n",
            "320 369\n",
            "320 370\n",
            "320 379\n",
            "320 380\n",
            "320 381\n",
            "320 383\n",
            "320 384\n",
            "320 386\n",
            "320 391\n",
            "320 427\n",
            "321 321\n",
            "326 326\n",
            "341 341\n",
            "376 376\n",
            "390 390\n",
            "456 455\n",
            "463 432\n",
            "465 465\n",
            "473 473\n",
            "479 479\n",
            "485 485\n",
            "486 485\n",
            "513 513\n",
            "520 483\n",
            "525 487\n",
            "536 524\n",
            "640 640\n",
            "696 525\n",
            "700 525\n",
            "1375 1375\n",
            "1399 1598\n",
            "1404 1404\n",
            "1440 1440\n",
            "1452 1452\n",
            "1480 1480\n",
            "1534 1724\n",
            "1594 1595\n",
            "1654 1654\n",
            "1714 1715\n",
            "1883 1884\n",
            "2029 2030\n",
            "2111 2110\n",
            "2391 2391\n",
            "2517 2518\n",
            "2532 2932\n",
            "2624 2623\n"
          ]
        }
      ],
      "source": [
        "unique_sizes = set()\n",
        "dataset_dir = data_path('dataset')\n",
        "for filename in os.listdir(dataset_dir):\n",
        "  if not filename.lower().endswith(\".jpg\"):\n",
        "    continue\n",
        "  fpath = os.path.join(dataset_dir, filename)\n",
        "  try:\n",
        "    with PIL.Image.open(fpath) as image:\n",
        "        w, h = image.size\n",
        "  except Exception as e:\n",
        "    print(\"Error reading\", fpath, e)\n",
        "    continue\n",
        "  unique_sizes.add((w, h))\n",
        "print(len(unique_sizes))\n",
        "for w, h in sorted(unique_sizes):\n",
        "  print(w, h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wbr0SBPuIi90"
      },
      "outputs": [],
      "source": [
        "def preprocess_folder(destination_directory,target_size):\n",
        "  os.makedirs(destination_directory, exist_ok=True)\n",
        "  dataset_dir = data_path('dataset')\n",
        "  for filename in os.listdir(dataset_dir):\n",
        "    if not filename.lower().endswith(\".jpg\"):\n",
        "      print(\"non jpg detected, eliminate dirty pngers\")\n",
        "      continue\n",
        "    source_path = os.path.join(dataset_dir, filename)\n",
        "    destination_path = os.path.join(destination_directory, filename)\n",
        "    try:\n",
        "      img = PIL.Image.open(source_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "      print(\"Error:\", source_path, e)\n",
        "      continue\n",
        "    img = img.resize(target_size, PIL.Image.LANCZOS)\n",
        "    img.save(destination_path, quality=95)\n",
        "#resizing the images to be 3 different sizes that are commonly compatible with networks layers (dividable by 8 16 n 32)\n",
        "preprocess_folder(data_path('preprocessed','dataset_low'), (64,64))\n",
        "preprocess_folder(data_path('preprocessed','dataset_mid'), (128,128))\n",
        "preprocess_folder(data_path('preprocessed','dataset_hig'), (256,256))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRezi5_m3mL8"
      },
      "source": [
        "need data augmentation\n",
        "add random rotations, cropping into the center of the image, resized crop, brightness and contrast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ROSkoK0LMcZb"
      },
      "outputs": [],
      "source": [
        "def random_rotate(img):\n",
        "  angle = random.uniform(-20, 20)\n",
        "  return img.rotate(angle, resample=PIL.Image.BILINEAR)\n",
        "\n",
        "def center_crop_resize(img, target_size):\n",
        "  width, height = img.size\n",
        "  min_side = min(width, height)\n",
        "  left = (width - min_side) // 2\n",
        "  top = (height - min_side) // 2\n",
        "  right = left + min_side\n",
        "  bottom = top + min_side\n",
        "  img_cropped = img.crop((left, top, right, bottom))\n",
        "  return img_cropped.resize(target_size, PIL.Image.BILINEAR)\n",
        "\n",
        "def random_resized_crop(img, target_size):\n",
        "  width, height = img.size\n",
        "  area = width * height\n",
        "  for _ in range(10):\n",
        "    target_area = random.uniform(0.9, 1.0) * area\n",
        "    log_ratio = (math.log(0.9), math.log(1.1))\n",
        "    aspect = math.exp(random.uniform(*log_ratio))\n",
        "    w = int(round(math.sqrt(target_area * aspect)))\n",
        "    h = int(round(math.sqrt(target_area / aspect)))\n",
        "    if 0 < w <= width and 0 < h <= height:\n",
        "      left = random.randint(0, width - w)\n",
        "      top = random.randint(0, height - h)\n",
        "      img_cropped = img.crop((left, top, left + w, top + h))\n",
        "      return img_cropped.resize(target_size, PIL.Image.BILINEAR)\n",
        "  return center_crop_resize(img, target_size)\n",
        "\n",
        "def random_brightness(img):\n",
        "  factor = 1.0 + random.uniform(-0.2, 0.2)\n",
        "  enhancer = PIL.ImageEnhance.Brightness(img)\n",
        "  return enhancer.enhance(factor)\n",
        "\n",
        "def random_contrast(img):\n",
        "  factor = 1.0 + random.uniform(-0.2, 0.2)\n",
        "  enhancer = PIL.ImageEnhance.Contrast(img)\n",
        "  return enhancer.enhance(factor)\n",
        "\n",
        "def augment_image_pil(img, target_size):\n",
        "  img = random_rotate(img)\n",
        "  img = random_resized_crop(img,target_size=target_size)\n",
        "  img = random_brightness(img)\n",
        "  img = random_contrast(img)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FNuzU6F4uhe"
      },
      "source": [
        "augment all 3 folders\n",
        "could have probably augmented then seperate into 3 resolutions, maybe test that later?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3hXP1cw0TS7-"
      },
      "outputs": [],
      "source": [
        "def augment_folder(source_directory, destination_directory, target_size, num_aug_per_image=1):\n",
        "  os.makedirs(destination_directory, exist_ok=True)\n",
        "  for fname in os.listdir(source_directory):\n",
        "    if not fname.lower().endswith(\".jpg\"):\n",
        "      continue\n",
        "    src_path = os.path.join(source_directory, fname)\n",
        "    try:\n",
        "      img = PIL.Image.open(src_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "      print(\"Error:\", src_path, e)\n",
        "      continue\n",
        "    base_name, ext = os.path.splitext(fname)\n",
        "    for i in range(num_aug_per_image):\n",
        "      aug_img = augment_image_pil(img, target_size=target_size)\n",
        "      new_name = f\"{base_name}_aug{i:02d}{ext}\"\n",
        "      dst_path = os.path.join(destination_directory, new_name)\n",
        "      aug_img.save(dst_path, quality=95)\n",
        "\n",
        "augment_folder(source_directory=data_path('preprocessed','dataset_low'),destination_directory=data_path('augmented','dataset_64_low'),target_size=(64, 64),num_aug_per_image=2)\n",
        "augment_folder(source_directory=data_path('preprocessed','dataset_mid'),destination_directory=data_path('augmented','dataset_128_mid'),target_size=(128, 128),num_aug_per_image=2)\n",
        "augment_folder(source_directory=data_path('preprocessed','dataset_hig'),destination_directory=data_path('augmented','dataset_256_hig'),target_size=(256, 256),num_aug_per_image=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "b4R8ApkNHsKh",
        "outputId": "6ea9f495-7e9d-41da-bc7c-9c9359d546ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading or displaying images: module 'matplotlib' has no attribute 'pyplot'\n"
          ]
        }
      ],
      "source": [
        "original_dir = data_path('preprocessed','dataset_low')\n",
        "augmented_dir = data_path('augmented','dataset_64_low')\n",
        "original_filenames = [f.split('.')[0] for f in os.listdir(original_dir) if f.lower().endswith(\".jpg\")]\n",
        "if not original_filenames:\n",
        "  print(f\"No original images found in {original_dir}\")\n",
        "else:\n",
        "  random_original_base = random.choice(original_filenames)\n",
        "  original_image_path = os.path.join(original_dir, f\"{random_original_base}.jpg\")\n",
        "\n",
        "  augmented_images = [\n",
        "    f for f in os.listdir(augmented_dir)\n",
        "    if f.startswith(random_original_base) and \"_aug\" in f and f.lower().endswith(\".jpg\")\n",
        "  ]\n",
        "\n",
        "  if not augmented_images:\n",
        "    print(f\"No augmented images found for {random_original_base} in {augmented_dir}\")\n",
        "  else:\n",
        "    random_augmented_filename = random.choice(augmented_images)\n",
        "    augmented_image_path = os.path.join(augmented_dir, random_augmented_filename)\n",
        "\n",
        "    try:\n",
        "      original_img = PIL.Image.open(original_image_path)\n",
        "      augmented_img = PIL.Image.open(augmented_image_path)\n",
        "\n",
        "      fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "      axes[0].imshow(original_img)\n",
        "      axes[0].set_title(f\"Original ({original_img.size[0]}x{original_img.size[1]})\\\\n{random_original_base}.jpg\")\n",
        "      axes[0].axis('off')\n",
        "\n",
        "      axes[1].imshow(augmented_img)\n",
        "      axes[1].set_title(f\"Augmented ({augmented_img.size[0]}x{augmented_img.size[1]})\\\\n{random_augmented_filename}\")\n",
        "      axes[1].axis('off')\n",
        "\n",
        "      matplotlib.pyplot.tight_layout()\n",
        "      matplotlib.pyplot.show()\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading or displaying images: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0ISnwLcWR5-"
      },
      "source": [
        "Newest dataset to be used is /data/augmented which has been preprocessed to have 3 versions. looking into it, cnn might be best on small amounts of data and mamba ssms on long sequences of data, given it's advantage is basically long context recall right? so i expect to see it do better on the 256 rather than the 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdnbEestPw6H"
      },
      "source": [
        "# training parameters and reusable functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EXZlvRahPiVa"
      },
      "outputs": [],
      "source": [
        "HYPERPARAMS = {\n",
        "  \"image_size\": 256, #64 128 256\n",
        "  \"num_classes\": 10,\n",
        "  \"batch_size\": 4,\n",
        "  \"val_ratio\": 0.2,\n",
        "  \"seed\": 42,\n",
        "  \"learning_rate\": 5e-5,\n",
        "  \"weight_decay\": 1e-4,\n",
        "  \"num_epochs\": 30,\n",
        "  \"d_model\": 128,\n",
        "  \"d_state\": 16,\n",
        "  \"num_mamba_layers\": 4,\n",
        "  \"mamba_patch_size\": 8,\n",
        "  \"dylan_patch_size\": 16,\n",
        "  \"num_dylan_layers\": 4,\n",
        "  \"dylan_ssm_expend\": 2,\n",
        "  \"dylan_dt_rank\": 16,\n",
        "  \"patience\": 5,\n",
        "  \"dropout\": 0.5\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4HFHE-_SP5-A"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(logits, labels):\n",
        "  one_hot = jax.nn.one_hot(labels, logits.shape[-1])\n",
        "  loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gka8m2bBP6EV"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(logits, labels):\n",
        "  preds = logits.argmax(axis=-1)\n",
        "  return (preds == labels).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0rTyFaDLnvd_"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(params, model_type=\"mamba\"):\n",
        "  if model_type == \"mamba\":\n",
        "    return optax.chain(optax.clip_by_global_norm(0.5),optax.adamw(learning_rate=params[\"learning_rate\"] * 0.5, weight_decay=params[\"weight_decay\"]))\n",
        "  else:\n",
        "    return optax.chain(optax.clip_by_global_norm(1.0),optax.adamw(learning_rate=params[\"learning_rate\"],weight_decay=params[\"weight_decay\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1H8oQD8rvdlm"
      },
      "outputs": [],
      "source": [
        "def creating_the_train_and_eval_functions(optimizer):\n",
        "  @equinox.filter_jit #jit this to speed it up\n",
        "  def train_step(model, opt_state, x, y, key):\n",
        "    keys = jax.random.split(key, x.shape[0])\n",
        "    def loss_function(m):\n",
        "      logits = jax.vmap(m, in_axes=(0, 0, None))(x, keys, True)\n",
        "      loss = cross_entropy_loss(logits, y)\n",
        "      return loss, logits\n",
        "\n",
        "    (loss, logits), grads = equinox.filter_value_and_grad(loss_function, has_aux=True)(model)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "    model = equinox.apply_updates(model, updates)\n",
        "    accuracy = compute_accuracy(logits, y)\n",
        "    return model, opt_state, loss, accuracy\n",
        "\n",
        "  @equinox.filter_jit\n",
        "  def eval_step(model, x, y):\n",
        "    logits = jax.vmap(model, in_axes=(0, None, None))(x, None, False)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "    accuracy = compute_accuracy(logits, y)\n",
        "    return loss, accuracy\n",
        "\n",
        "  return train_step, eval_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eBxJMESXn1Uy"
      },
      "outputs": [],
      "source": [
        "class JAXDataLoader: #loading the data using jax might be faster, also include the training\n",
        "  def __init__(self,loader):\n",
        "    self.loader = loader\n",
        "\n",
        "  def __iter__(self):\n",
        "    for batch in self.loader:\n",
        "      x,y = batch\n",
        "      x_numpy = x.cpu().numpy() if torch.is_tensor(x) else numpy.array(x)\n",
        "      y_numpy = y.cpu().numpy() if torch.is_tensor(y) else numpy.array(y)\n",
        "      x_jax = jax.numpy.array(x_numpy)\n",
        "      y_jax = jax.numpy.array(y_numpy)\n",
        "      yield x_jax, y_jax\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.loader)\n",
        "\n",
        "  def train_model(model, train_loader, val_loader, params, model_name):\n",
        "    optimizer = get_optimizer(params)\n",
        "    opt_state = optimizer.init(equinox.filter(model, equinox.is_inexact_array))\n",
        "\n",
        "    @equinox.filter_jit #jit this\n",
        "    def train_step(model, opt_state, x, y, key):\n",
        "      keys = jax.random.split(key, x.shape[0])\n",
        "      def loss_function(m):\n",
        "        logits = jax.vmap(m, in_axes=(0,0,None))(x,keys,True)\n",
        "        loss = cross_entropy_loss(logits,y)\n",
        "        return loss, logits\n",
        "      (loss,logits),grads = equinox.filter_value_and_grad(loss_function, has_aux=True)(model)\n",
        "      updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "      model = equinox.apply_updates(model, updates)\n",
        "      accuracy = compute_accuracy(logits, y)\n",
        "      return model, opt_state, loss, accuracy\n",
        "\n",
        "    @equinox.filter_jit #jit this\n",
        "    def eval_step(model, x, y):\n",
        "      logits = jax.vmap(model, in_axes=(0,None,None))(x,None,False)\n",
        "      loss = cross_entropy_loss(logits,y)\n",
        "      accuracy = compute_accuracy(logits, y) # Fixed: call compute_accuracy\n",
        "      return loss, accuracy\n",
        "\n",
        "    train_loader_jax = JAXDataLoader(train_loader)\n",
        "    val_loader_jax = JAXDataLoader(val_loader)\n",
        "\n",
        "    key = jax.random.PRNGKey(params[\"seed\"])\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "    patience = params[\"patience\"]\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_model = model\n",
        "    counter = 0\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, params[\"num_epochs\"]+1):\n",
        "      epoch_start = time.time()\n",
        "      epoch_train_loss = []\n",
        "      epoch_train_accuracy = []\n",
        "      for x_batch, y_batch in train_loader_jax:\n",
        "        key,subkey = jax.random.split(key)\n",
        "        model,opt_state,loss,accuracy = train_step(model,opt_state,x_batch,y_batch,subkey)\n",
        "        epoch_train_loss.append(loss.item())\n",
        "        epoch_train_accuracy.append(accuracy.item())\n",
        "      epoch_val_loss=[]\n",
        "      epoch_val_accuracy=[]\n",
        "      for x_batch, y_batch in val_loader_jax:\n",
        "        loss, accuracy = eval_step(model, x_batch, y_batch)\n",
        "        epoch_val_loss.append(loss.item())\n",
        "        epoch_val_accuracy.append(accuracy.item())\n",
        "      average_train_loss = numpy.mean(epoch_train_loss)\n",
        "      average_train_accuracy = numpy.mean(epoch_train_accuracy)\n",
        "      average_val_loss = numpy.mean(epoch_val_loss)\n",
        "      average_val_accuracy = numpy.mean(epoch_val_accuracy)\n",
        "      train_losses.append(average_train_loss)\n",
        "      val_losses.append(average_val_loss)\n",
        "      train_accuracies.append(average_train_accuracy)\n",
        "      val_accuracies.append(average_val_accuracy)\n",
        "      epoch_time = time.time() - epoch_start\n",
        "      print(f\"epoch{epoch} avgtrainingloss{average_train_loss:.4f} avgtrainingaccuracy{average_train_accuracy:.4f} avgvalloss{average_val_loss:.4f} avgvalaccuracy{average_val_accuracy:.4f} time{epoch_time:.1f}\")\n",
        "      if average_val_loss < best_val_loss:\n",
        "        best_val_loss = average_val_loss\n",
        "        best_model = model\n",
        "        counter = 0\n",
        "      else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "          print(f\" early stopping at{epoch}\")\n",
        "          break\n",
        "    total_time = time.time() - start_time\n",
        "    print(total_time)\n",
        "    history = {\"train_loss\": train_losses, \"val_loss\": val_losses, \"train_accuracy\": train_accuracies, \"val_accuracy\": val_accuracies, \"time\": total_time}\n",
        "    return best_model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x93J0UmTAcLQ"
      },
      "outputs": [],
      "source": [
        "def parse_label_from_name(filename):\n",
        "  base, _ = os.path.splitext(filename)\n",
        "  parts = base.split(\"_\", 1)\n",
        "  label_part = parts[1]\n",
        "  if \"_aug\" in label_part:\n",
        "    label_part = label_part.split(\"_aug\", 1)[0]\n",
        "  return label_part\n",
        "\n",
        "\n",
        "def build_index_flat(root_directory, seed=0):\n",
        "  all_files = [f for f in os.listdir(root_directory) if f.lower().endswith(\".jpg\")]\n",
        "  labels = [parse_label_from_name(f) for f in all_files]\n",
        "  class_names = sorted(set(labels))\n",
        "  label2idx = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "  samples = []\n",
        "  for filename, lbl in zip(all_files, labels):\n",
        "    path = os.path.join(root_directory, filename)\n",
        "    samples.append((path, label2idx[lbl]))\n",
        "\n",
        "  random.Random(seed).shuffle(samples)\n",
        "  n_total = len(samples)\n",
        "  n_val = int(n_total * 0.2)\n",
        "  val_samples = samples[:n_val]\n",
        "  train_samples = samples[n_val:]\n",
        "  return train_samples, val_samples, label2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8ogyGS9Pa_h8"
      },
      "outputs": [],
      "source": [
        "def convert_equinox_to_flax_batch(batch): #function to try to incorporate groupmate's work in flax to be usable without equinox workflow\n",
        "  images, labels = batch\n",
        "  images_np = jax.numpy.array(images.numpy())\n",
        "  if images_np.shape[-1] == 3:\n",
        "    images_flax = images_np\n",
        "  else:\n",
        "    images_flax = jax.numpy.transpose(images_np, (0, 2, 3, 1))\n",
        "  labels_np = jax.numpy.array(labels.numpy())\n",
        "  return images_flax, labels_np\n",
        "\n",
        "def train_flax_model(model_class, train_loader, val_loader, params, model_name):\n",
        "  import time\n",
        "  from flax.training import train_state\n",
        "  import optax\n",
        "  config = params\n",
        "  num_classes = config[\"num_classes\"]\n",
        "  rng = jax.random.PRNGKey(config[\"seed\"])\n",
        "  rng, init_rng = jax.random.split(rng)\n",
        "  h, w = config[\"image_size\"], config[\"image_size\"]\n",
        "  dummy_input = jax.numpy.ones((1, h, w, 3))\n",
        "  model = model_class(num_classes=num_classes)\n",
        "  variables = model.init(init_rng, dummy_input, train=False)\n",
        "  tx = optax.adamw(learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "  state = train_state.TrainState.create(apply_fn=model.apply,params=variables['params'],tx=tx)\n",
        "  batch_stats = variables.get('batch_stats', None)\n",
        "\n",
        "  @jax.jit\n",
        "  def train_step(state, batch_stats, images, labels, dropout_rng):\n",
        "    def loss_fn(model_params):\n",
        "      if batch_stats is not None:\n",
        "        output = state.apply_fn(\n",
        "          {'params': model_params, 'batch_stats': batch_stats},\n",
        "          images,\n",
        "          train=True,\n",
        "          mutable=['batch_stats'],\n",
        "          rngs={'dropout': dropout_rng}\n",
        "        )\n",
        "        logits, mutated = output\n",
        "        new_batch_stats = mutated['batch_stats']\n",
        "      else:\n",
        "        logits = state.apply_fn(\n",
        "          {'params': model_params},\n",
        "          images,\n",
        "          train=True,\n",
        "          rngs={'dropout': dropout_rng}\n",
        "        )\n",
        "        new_batch_stats = None\n",
        "\n",
        "      labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)\n",
        "      loss = -jax.numpy.mean(jax.numpy.sum(labels_onehot * jax.nn.log_softmax(logits), axis=-1))\n",
        "      acc = jax.numpy.mean(jax.numpy.argmax(logits, axis=-1) == labels)\n",
        "      return loss, (acc, new_batch_stats)\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, (acc, new_batch_stats)), grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, new_batch_stats, loss, acc\n",
        "\n",
        "  @jax.jit\n",
        "  def eval_step(model_params, batch_stats, images, labels):\n",
        "    if batch_stats is not None:\n",
        "      logits = state.apply_fn(\n",
        "        {'params': model_params, 'batch_stats': batch_stats},\n",
        "        images,\n",
        "        train=False\n",
        "      )\n",
        "    else:\n",
        "      logits = state.apply_fn(\n",
        "        {'params': model_params},\n",
        "        images,\n",
        "        train=False\n",
        "      )\n",
        "\n",
        "    labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)\n",
        "    loss = -jax.numpy.mean(jax.numpy.sum(labels_onehot * jax.nn.log_softmax(logits), axis=-1))\n",
        "    acc = jax.numpy.mean(jax.numpy.argmax(logits, axis=-1) == labels)\n",
        "    return loss, acc\n",
        "\n",
        "  history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'time': 0}\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(config[\"num_epochs\"]):\n",
        "    train_loss, train_acc, n_train = 0.0, 0.0, 0\n",
        "    for batch in train_loader:\n",
        "      images, labels = convert_equinox_to_flax_batch(batch)\n",
        "      rng, dropout_rng = jax.random.split(rng)\n",
        "      state, batch_stats, loss, acc = train_step(state, batch_stats, images, labels, dropout_rng)\n",
        "      train_loss += loss\n",
        "      train_acc += acc\n",
        "      n_train += 1\n",
        "\n",
        "    val_loss, val_acc, n_val = 0.0, 0.0, 0\n",
        "    for batch in val_loader:\n",
        "      images, labels = convert_equinox_to_flax_batch(batch)\n",
        "      loss, acc = eval_step(state.params, batch_stats, images, labels)\n",
        "      val_loss += loss\n",
        "      val_acc += acc\n",
        "      n_val += 1\n",
        "\n",
        "    history['train_loss'].append(train_loss/n_train)\n",
        "    history['train_acc'].append(train_acc/n_train)\n",
        "    history['val_loss'].append(val_loss/n_val)\n",
        "    history['val_acc'].append(val_acc/n_val)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} TLoss: {train_loss/n_train:.4f} Acc: {train_acc/n_train:.4f} VLoss: {val_loss/n_val:.4f} Acc: {val_acc/n_val:.4f}\")\n",
        "\n",
        "  history['time'] = time.time() - start_time\n",
        "  print(f\"{model_name} Time: {history['time']:.2f}s\\n\")\n",
        "  return state, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AnBH0CccLy4-"
      },
      "outputs": [],
      "source": [
        "class WoundDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,samples):\n",
        "    self.samples = samples\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "  def __getitem__(self,idx):\n",
        "    path, label = self.samples[idx]\n",
        "    try:\n",
        "      with PIL.Image.open(path) as img:\n",
        "        img = img.convert(\"RGB\")\n",
        "        image_numpy = numpy.array(img).astype(numpy.float32)/255\n",
        "    except Exception as e:\n",
        "      print(\"image loading f-ed up\")\n",
        "      image_numpy = numpy.zeros((64,64,3), dtype=numpy.float32)\n",
        "    return image_numpy,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQmu9qBYA3zQ",
        "outputId": "396ae481-078e-4837-ff36-a574a8c58e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classes: {'Abrasions': 0, 'Bruises': 1, 'Burns': 2, 'Cut': 3, 'Diabetic_Wounds': 4, 'Laseration': 5, 'Normal': 6, 'Pressure_Wounds': 7, 'Surgical_Wounds': 8, 'Venous_Wounds': 9}\n",
            "train samples: 2352\n",
            "val samples: 588\n"
          ]
        }
      ],
      "source": [
        "if HYPERPARAMS[\"image_size\"] == 64:\n",
        "  data_directory = data_path('augmented','dataset_64_low')\n",
        "elif HYPERPARAMS[\"image_size\"] == 128:\n",
        "  data_directory = data_path('augmented','dataset_128_mid')\n",
        "elif HYPERPARAMS[\"image_size\"] == 256:\n",
        "  data_directory = data_path('augmented','dataset_256_hig')\n",
        "else:\n",
        "  raise ValueError(\"image size?\")\n",
        "\n",
        "train_samples, val_samples, label2idx = build_index_flat(data_directory, seed=42)\n",
        "num_classes = len(label2idx)\n",
        "\n",
        "config = HYPERPARAMS.copy()\n",
        "config[\"num_classes\"] = num_classes\n",
        "\n",
        "print(\"classes:\", label2idx)\n",
        "print(\"train samples:\", len(train_samples))\n",
        "print(\"val samples:\", len(val_samples))\n",
        "\n",
        "train_datast = WoundDataset(train_samples)\n",
        "val_dataset = WoundDataset(val_samples)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  train_datast,\n",
        "  batch_size=HYPERPARAMS[\"batch_size\"],\n",
        "  shuffle=True,\n",
        "  num_workers=0,\n",
        "  drop_last=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "  val_dataset,\n",
        "  batch_size=HYPERPARAMS[\"batch_size\"],\n",
        "  shuffle=True,\n",
        "  num_workers=0,\n",
        "  drop_last=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7cK7_5j_qNa"
      },
      "source": [
        "# actual model and stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JMeiRAPmwQAD"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalBlock(equinox.Module):\n",
        "  conv: equinox.nn.Conv2d\n",
        "  norm: equinox.nn.GroupNorm\n",
        "  act: equinox.nn.Lambda\n",
        "\n",
        "  def __init__(self, in_ch, out_ch, *, key):\n",
        "    k1, k2 = jax.random.split(key, 2)\n",
        "    self.conv = equinox.nn.Conv2d(\n",
        "      in_channels=in_ch,\n",
        "      out_channels=out_ch,\n",
        "      kernel_size=3,\n",
        "      padding=1,\n",
        "      key=k1,\n",
        "    )\n",
        "    groups=32\n",
        "    self.norm = equinox.nn.GroupNorm(groups=groups, channels=out_ch)\n",
        "    self.act = equinox.nn.Lambda(jax.nn.relu)\n",
        "\n",
        "  def __call__(self, x, *, key=None):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0-CuCLLhL1Qa"
      },
      "outputs": [],
      "source": [
        "class Baseline(equinox.Module):\n",
        "  conv_blocks: Sequence[ConvolutionalBlock]\n",
        "  fc1: equinox.nn.Linear\n",
        "  fc2: equinox.nn.Linear\n",
        "  fc3: equinox.nn.Linear\n",
        "  dropout: equinox.nn.Dropout\n",
        "\n",
        "  def __init__(self, image_size, in_ch, num_classes, *, key):\n",
        "\n",
        "    k_conv, k_fc1, k_fc2, k_fc3, k_do = jax.random.split(key, 5)\n",
        "\n",
        "    channels = [in_ch, 32, 64, 128, 256]\n",
        "    blocks = []\n",
        "    k = k_conv\n",
        "    for i in range(4):\n",
        "      k, subk = jax.random.split(k)\n",
        "      blocks.append(ConvolutionalBlock(channels[i], channels[i+1], key=subk))\n",
        "    self.conv_blocks = tuple(blocks)\n",
        "    spatial = image_size // 16\n",
        "    flattened_dim = channels[-1] * spatial * spatial\n",
        "\n",
        "    self.fc1 = equinox.nn.Linear(flattened_dim, 256, key=k_fc1)\n",
        "    self.fc2 = equinox.nn.Linear(256, 128, key=k_fc2)\n",
        "    self.fc3 = equinox.nn.Linear(128, num_classes, key=k_fc3)\n",
        "    self.dropout = equinox.nn.Dropout(p=HYPERPARAMS[\"dropout\"]) #changed to 0.5 becuz 0.5 was a bit too high?\n",
        "\n",
        "  def __call__(self, x, key=None, train=True):\n",
        "    x = jax.numpy.transpose(x, (2,0,1))\n",
        "\n",
        "    for block in self.conv_blocks:\n",
        "      x = block(x)\n",
        "      x = jax.lax.reduce_window(\n",
        "        x,\n",
        "        -jax.numpy.inf,\n",
        "        jax.lax.max,\n",
        "        window_dimensions=(1,2,2),\n",
        "        window_strides=(1,2,2),\n",
        "        padding=\"VALID\",\n",
        "      )\n",
        "\n",
        "    x = x.reshape(-1)\n",
        "\n",
        "    if key is None:\n",
        "      key = jax.random.PRNGKey(0)\n",
        "    k1, k2 = jax.random.split(key, 2)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = jax.nn.relu(x) #using relu because even deepseek v3.2 uses relu lmao\n",
        "    x = self.dropout(x, key=k1, inference=not train)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = self.dropout(x, key=k2, inference=not train)\n",
        "\n",
        "    logits = self.fc3(x)\n",
        "    return logits\n",
        "\n",
        "  def output_shape(self, image_size):\n",
        "    spatial = image_size // 16\n",
        "    return (self.fc3.out_features, spatial, spatial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7a-3x9fOuEXq"
      },
      "outputs": [],
      "source": [
        "class MambaBlock(equinox.Module):\n",
        "  d_model: int\n",
        "  d_state: int\n",
        "  proj_B: equinox.nn.Linear\n",
        "  proj_C: equinox.nn.Linear\n",
        "  proj_delta: equinox.nn.Linear\n",
        "  A_log: jax.Array\n",
        "  D: equinox.nn.Linear\n",
        "  in_proj: equinox.nn.Linear\n",
        "  out_proj: equinox.nn.Linear\n",
        "  norm: equinox.nn.LayerNorm\n",
        "\n",
        "  def __init__(self, d_model, d_state, key):\n",
        "    keys = jax.random.split(key, 7)\n",
        "    self.d_model = d_model\n",
        "    self.d_state = d_state\n",
        "    self.norm = equinox.nn.LayerNorm(d_model)\n",
        "    self.proj_B = equinox.nn.Linear(d_model, d_state * d_model, key=keys[0])\n",
        "    self.proj_C = equinox.nn.Linear(d_model, d_model * d_state, key=keys[1])\n",
        "    self.proj_delta = equinox.nn.Linear(d_model, d_state, key=keys[2])\n",
        "    self.A_log = jax.nn.initializers.normal(0.01)(keys[3], (d_state,))\n",
        "    self.A_log = jax.numpy.log(jax.numpy.exp(self.A_log) + 1e-4)\n",
        "    self.D = equinox.nn.Linear(d_model, d_model, key=keys[4])\n",
        "    self.in_proj = equinox.nn.Linear(d_model, d_model * 2, key=keys[5])\n",
        "    self.out_proj = equinox.nn.Linear(d_model, d_model, key=keys[6])\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x_norm = jax.vmap(self.norm)(x)\n",
        "    x_proj = jax.vmap(self.in_proj)(x_norm)\n",
        "    x_main, x_gate = jax.numpy.split(x_proj, 2, axis=-1)\n",
        "    x_main = jax.numpy.tanh(x_main)\n",
        "    L = x.shape[0]\n",
        "    B = jax.vmap(self.proj_B)(x_main)\n",
        "    B = jax.numpy.reshape(B, (L, self.d_state, self.d_model))\n",
        "    C = jax.vmap(self.proj_C)(x_main)\n",
        "    C = jax.numpy.reshape(C, (L, self.d_model, self.d_state))\n",
        "    delta = jax.vmap(self.proj_delta)(x_main)\n",
        "    delta = jax.nn.softplus(delta)\n",
        "    delta = jax.numpy.clip(delta, 1e-3, 10.0)\n",
        "    A = -jax.numpy.exp(self.A_log)\n",
        "    A = jax.numpy.clip(A, -0.99, -1e-4)\n",
        "    A_bar = jax.numpy.exp(A * delta)\n",
        "    s = jax.numpy.where(\n",
        "      jax.numpy.abs(A) < 1e-8,\n",
        "      delta,\n",
        "      (jax.numpy.exp(A * delta) - 1) / (A + 1e-8)\n",
        "    )\n",
        "    B_bar = s[:, :, None] * B\n",
        "    D_x = jax.vmap(self.D)(x_main)\n",
        "\n",
        "    def scan_fn(h, params):\n",
        "      A_bar_t, B_bar_t, C_t, x_main_t, D_x_t = params\n",
        "      u_t = jax.numpy.dot(B_bar_t, x_main_t)\n",
        "      h_new = A_bar_t * h + u_t\n",
        "      y_t = jax.numpy.dot(C_t, h_new) + D_x_t\n",
        "      return h_new, y_t\n",
        "\n",
        "    h_init = jax.numpy.zeros((self.d_state,))\n",
        "    xs = (A_bar, B_bar, C, x_main, D_x)\n",
        "    _, y = jax.lax.scan(scan_fn, h_init, xs)\n",
        "    y = y * jax.nn.silu(x_gate) #official implementation uses silu not relu\n",
        "    y = jax.vmap(self.out_proj)(y)\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2xCX_m9DAUdb"
      },
      "outputs": [],
      "source": [
        "class ImagePatcher(equinox.Module):\n",
        "  #because mamba only works with sequences, so we have to turn image into patches similar to sequences\n",
        "  patch_size: int\n",
        "  d_model: int\n",
        "  projection: equinox.nn.Linear\n",
        "\n",
        "  def __init__(self, patch_size, d_model, key):\n",
        "    self.patch_size = patch_size\n",
        "    self.d_model = d_model\n",
        "    patch_dim = patch_size * patch_size * 3\n",
        "    self.projection = equinox.nn.Linear(patch_dim, d_model, key=key)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    h, w, c = x.shape\n",
        "    p = self.patch_size\n",
        "    n_patches_h = h // p\n",
        "    n_patches_w = w // p\n",
        "    x = x[:n_patches_h*p, :n_patches_w*p, :]\n",
        "    patches = jax.numpy.reshape(x, (n_patches_h, p, n_patches_w, p, c))\n",
        "    patches = jax.numpy.transpose(patches, (0, 2, 1, 3, 4))\n",
        "    patches = jax.numpy.reshape(patches, (n_patches_h * n_patches_w, p * p * c))\n",
        "    return jax.vmap(self.projection)(patches)\n",
        "\n",
        "  def num_patches(self, image_size: int):\n",
        "    p = self.patch_size\n",
        "    return (image_size // p) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CqP3wkhjAS5R"
      },
      "outputs": [],
      "source": [
        "class UnidirectionalMamba(equinox.Module):\n",
        "  patcher: ImagePatcher\n",
        "  blocks: list\n",
        "  norm: equinox.nn.LayerNorm\n",
        "  dropout: equinox.nn.Dropout\n",
        "  classifier: equinox.nn.Linear\n",
        "\n",
        "  def __init__(self, patch_size, num_layers, d_model, d_state, num_classes, *, key, image_size):\n",
        "    keys = jax.random.split(key, num_layers + 4)\n",
        "    self.patcher = ImagePatcher(patch_size, d_model, keys[0])\n",
        "    self.blocks = [MambaBlock(d_model, d_state, keys[i+1]) for i in range(num_layers)]\n",
        "    self.norm = equinox.nn.LayerNorm(d_model)\n",
        "    self.dropout = equinox.nn.Dropout(p=HYPERPARAMS[\"dropout\"])\n",
        "    self.classifier = equinox.nn.Linear(d_model, num_classes, key=keys[-1])\n",
        "\n",
        "  def __call__(self, x, key=None, train=True):\n",
        "    x = self.patcher(x)\n",
        "    for block in self.blocks:\n",
        "      x = block(x) + x\n",
        "    x = jax.vmap(self.norm)(x)\n",
        "    x = jax.numpy.mean(x, axis=0)\n",
        "    if key is None:\n",
        "      key = jax.random.PRNGKey(0)\n",
        "    x = self.dropout(x, key=key, inference=not train)\n",
        "    logits = self.classifier(x)\n",
        "    return logits\n",
        "\n",
        "  def num_patches(self, image_size: int):\n",
        "    return self.patcher.num_patches(image_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "P4ZsNiRybAXW"
      },
      "outputs": [],
      "source": [
        "class BidirectionalMamba(equinox.Module):\n",
        "  patcher: ImagePatcher\n",
        "  forward_blocks: list\n",
        "  backward_blocks: list\n",
        "  norm: equinox.nn.LayerNorm\n",
        "  dropout: equinox.nn.Dropout\n",
        "  classifier: equinox.nn.Linear\n",
        "  pos_embed: jax.Array\n",
        "\n",
        "  def __init__(self, patch_size, num_layers, d_model, d_state, num_classes, *, key, image_size):\n",
        "    keys = jax.random.split(key, 5)\n",
        "    key_patcher, key_pos, key_fwd, key_bwd, key_cls = keys\n",
        "    self.patcher = ImagePatcher(patch_size, d_model, key_patcher)\n",
        "    fwd_keys = jax.random.split(key_fwd, num_layers)\n",
        "    bwd_keys = jax.random.split(key_bwd, num_layers)\n",
        "    self.forward_blocks = [MambaBlock(d_model, d_state, k) for k in fwd_keys]\n",
        "    self.backward_blocks = [MambaBlock(d_model, d_state, k) for k in bwd_keys]\n",
        "    self.norm = equinox.nn.LayerNorm(d_model)\n",
        "    self.dropout = equinox.nn.Dropout(p=HYPERPARAMS[\"dropout\"])\n",
        "    self.classifier = equinox.nn.Linear(d_model, num_classes, key=key_cls)\n",
        "    num_patches = self.patcher.num_patches(image_size)\n",
        "    self.pos_embed = jax.random.normal(key_pos, (num_patches, d_model)) * 0.02\n",
        "\n",
        "  def __call__(self, x, key=None, train=True):\n",
        "    x = self.patcher(x)\n",
        "    x = x + self.pos_embed\n",
        "    for f_block, b_block in zip(self.forward_blocks, self.backward_blocks):\n",
        "      out_forward = f_block(x)\n",
        "      x_flip = jax.numpy.flip(x, axis=0)\n",
        "      out_backward_flip = b_block(x_flip)\n",
        "      out_backward = jax.numpy.flip(out_backward_flip, axis=0)\n",
        "      out_combined = (out_forward + out_backward) / 2\n",
        "      x = out_combined + x\n",
        "    x = jax.vmap(self.norm)(x)\n",
        "    x = jax.numpy.mean(x, axis=0)\n",
        "    if key is None:\n",
        "      key = jax.random.PRNGKey(0)\n",
        "    x = self.dropout(x, key=key, inference=not train)\n",
        "    logits = self.classifier(x)\n",
        "    return logits\n",
        "\n",
        "  def num_patches(self, image_size: int):\n",
        "    return self.patcher.num_patches(image_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-rbPt7YYwO2"
      },
      "source": [
        "## some code from dylan's previous project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-75hKTPMY1cR"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(flax.linen.Module):\n",
        "  in_channels: int\n",
        "  mid_channels: int\n",
        "  out_channels: int\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train=True):\n",
        "    x = flax.linen.Conv(self.mid_channels, kernel_size=(3, 3), padding='SAME', use_bias=False)(x)\n",
        "    x = flax.linen.BatchNorm(use_running_average=not train)(x)\n",
        "    x = flax.linen.relu(x)\n",
        "    x = flax.linen.Conv(self.out_channels, kernel_size=(3, 3), padding='SAME', use_bias=False)(x)\n",
        "    x = flax.linen.BatchNorm(use_running_average=not train)(x)\n",
        "    x = flax.linen.relu(x)\n",
        "    return x\n",
        "\n",
        "class Down(flax.linen.Module):\n",
        "  in_channels: int\n",
        "  out_channels: int\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train=True):\n",
        "    x = flax.linen.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = DoubleConv(self.in_channels, self.out_channels, self.out_channels)(x, train=train)\n",
        "    return x\n",
        "\n",
        "class Up(flax.linen.Module):\n",
        "  in_channels: int\n",
        "  out_channels: int\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x1, x2, train=True):\n",
        "    x1 = jax.image.resize(x1, shape=(x1.shape[0], x2.shape[1], x2.shape[2], x1.shape[3]), method='bilinear')\n",
        "    x = jax.numpy.concatenate([x2, x1], axis=-1)\n",
        "    x = DoubleConv(self.in_channels, self.out_channels, self.out_channels)(x, train=train)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b6WYEHxsZcNo"
      },
      "outputs": [],
      "source": [
        "class FlaxUNet(flax.linen.Module):  # Renamed to FlaxUNet to avoid conflict\n",
        "  \"\"\"UNet for Classification (Modified from segmentation)\"\"\"\n",
        "  num_classes: int = 9  # Modified for classification\n",
        "  nb_filter: Sequence[int] = (64, 128, 256, 512, 1024)\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train: bool = True):\n",
        "    \"\"\" :param x:  (N, H, W, C) :param train:  :return: Classification logits (N, num_classes)\"\"\"\n",
        "    in_channels = x.shape[-1]\n",
        "    x1 = DoubleConv(in_channels, self.nb_filter[0], self.nb_filter[0])(x, train=train)\n",
        "    x2 = Down(self.nb_filter[0], self.nb_filter[1])(x1, train=train)\n",
        "    x3 = Down(self.nb_filter[1], self.nb_filter[2])(x2, train=train)\n",
        "    x4 = Down(self.nb_filter[2], self.nb_filter[3])(x3, train=train)\n",
        "    x5 = Down(self.nb_filter[3], self.nb_filter[4])(x4, train=train)\n",
        "\n",
        "    # Decoder\n",
        "    x = Up(self.nb_filter[4], self.nb_filter[3])(x5, x4, train=train)\n",
        "    x = Up(self.nb_filter[3], self.nb_filter[2])(x, x3, train=train)\n",
        "    x = Up(self.nb_filter[2], self.nb_filter[1])(x, x2, train=train)\n",
        "    x = Up(self.nb_filter[1], self.nb_filter[0])(x, x1, train=train)\n",
        "\n",
        "    # MODIFIED: Classification Head instead of Segmentation Output\n",
        "    x = jax.numpy.mean(x, axis=(1, 2))\n",
        "    x = flax.linen.Dense(256)(x)\n",
        "    x = flax.linen.relu(x)\n",
        "    x = flax.linen.Dropout(rate=0.5, deterministic=not train)(x)\n",
        "    x = flax.linen.Dense(self.num_classes)(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dhJ8cvrqidcc"
      },
      "outputs": [],
      "source": [
        "# DoubleConv class is same as above, but kept here for standalone functionality\n",
        "class FlaxDoubleConv(flax.linen.Module):  # Renamed to avoid conflict\n",
        "  inChannels: int\n",
        "  middleChannels: int\n",
        "  outChannels: int\n",
        "  preBatchNorm: bool = False\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train: bool = True):\n",
        "    axis = -1\n",
        "    if self.preBatchNorm:\n",
        "      x = flax.linen.BatchNorm(use_running_average=not train, momentum=0.9, epsilon=1e-5, axis=axis)(x)\n",
        "      x = flax.linen.relu(x)\n",
        "      x = flax.linen.Conv(features=self.middleChannels, kernel_size=(3, 3), padding=\"SAME\", use_bias=False)(x)\n",
        "      x = flax.linen.BatchNorm(use_running_average=not train, momentum=0.9, epsilon=1e-5, axis=axis)(x)\n",
        "      x = flax.linen.relu(x)\n",
        "      x = flax.linen.Conv(features=self.outChannels, kernel_size=(3, 3), padding=\"SAME\", use_bias=False)(x)\n",
        "    else:\n",
        "      x = flax.linen.Conv(features=self.middleChannels, kernel_size=(3, 3), padding=\"SAME\", use_bias=False)(x)\n",
        "      x = flax.linen.BatchNorm(use_running_average=not train, momentum=0.9, epsilon=1e-5, axis=axis)(x)\n",
        "      x = flax.linen.relu(x)\n",
        "      x = flax.linen.Conv(features=self.outChannels, kernel_size=(3, 3), padding=\"SAME\", use_bias=False)(x)\n",
        "      x = flax.linen.BatchNorm(use_running_average=not train, momentum=0.9, epsilon=1e-5, axis=axis)(x)\n",
        "      x = flax.linen.relu(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Backbone(flax.linen.Module):\n",
        "  nb_filter: Sequence[int]\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train: bool = True):\n",
        "    x0_0 = FlaxDoubleConv(x.shape[-1], self.nb_filter[0], self.nb_filter[0])(x, train=train)\n",
        "    x1_0 = flax.linen.max_pool(x0_0, window_shape=(2, 2), strides=(2, 2))\n",
        "    x1_0 = FlaxDoubleConv(self.nb_filter[0], self.nb_filter[1], self.nb_filter[1])(x1_0, train=train)\n",
        "    x2_0 = flax.linen.max_pool(x1_0, window_shape=(2, 2), strides=(2, 2))\n",
        "    x2_0 = FlaxDoubleConv(self.nb_filter[1], self.nb_filter[2], self.nb_filter[2])(x2_0, train=train)\n",
        "    x3_0 = flax.linen.max_pool(x2_0, window_shape=(2, 2), strides=(2, 2))\n",
        "    x3_0 = FlaxDoubleConv(self.nb_filter[2], self.nb_filter[3], self.nb_filter[3])(x3_0, train=train)\n",
        "    x4_0 = flax.linen.max_pool(x3_0, window_shape=(2, 2), strides=(2, 2))\n",
        "    x4_0 = FlaxDoubleConv(self.nb_filter[3], self.nb_filter[4], self.nb_filter[4])(x4_0, train=train)\n",
        "    return x0_0, x1_0, x2_0, x3_0, x4_0\n",
        "\n",
        "\n",
        "class FlaxUp(flax.linen.Module): # Renamed\n",
        "  nb_filter: int\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x):\n",
        "    new_shape = (x.shape[0], x.shape[1] * 2, x.shape[2] * 2, x.shape[3])\n",
        "    x = jax.image.resize(x, shape=new_shape, method=\"bilinear\")\n",
        "    return x\n",
        "\n",
        "\n",
        "class FlaxNestedUNet(flax.linen.Module): # Renamed\n",
        "  \"\"\"\n",
        "  NestedUNet (UNet++) for Classification\n",
        "  \"\"\"\n",
        "  num_classes: int = 9\n",
        "  nb_filter: Sequence[int] = (32, 64, 128, 256, 512)\n",
        "  deep_supervision: bool = False\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train: bool = True):\n",
        "    x0_0, x1_0, x2_0, x3_0, x4_0 = Backbone(nb_filter=self.nb_filter)(x, train=train)\n",
        "\n",
        "    x0_1 = FlaxUp(nb_filter=self.nb_filter[0])(x1_0)\n",
        "    x0_1 = jax.numpy.concatenate([x0_0, x0_1], axis=-1)\n",
        "    x0_1 = FlaxDoubleConv(self.nb_filter[0] + self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])(x0_1, train=train)\n",
        "\n",
        "    x1_1 = FlaxUp(nb_filter=self.nb_filter[1])(x2_0)\n",
        "    x1_1 = jax.numpy.concatenate([x1_0, x1_1], axis=-1)\n",
        "    x1_1 = FlaxDoubleConv(self.nb_filter[1] + self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])(x1_1, train=train)\n",
        "\n",
        "    x2_1 = FlaxUp(nb_filter=self.nb_filter[2])(x3_0)\n",
        "    x2_1 = jax.numpy.concatenate([x2_0, x2_1], axis=-1)\n",
        "    x2_1 = FlaxDoubleConv(self.nb_filter[2] + self.nb_filter[3], self.nb_filter[2], self.nb_filter[2])(x2_1, train=train)\n",
        "\n",
        "    x3_1 = FlaxUp(nb_filter=self.nb_filter[3])(x4_0)\n",
        "    x3_1 = jax.numpy.concatenate([x3_0, x3_1], axis=-1)\n",
        "    x3_1 = FlaxDoubleConv(self.nb_filter[3] + self.nb_filter[4], self.nb_filter[3], self.nb_filter[3])(x3_1, train=train)\n",
        "\n",
        "    x0_2 = FlaxUp(nb_filter=self.nb_filter[0])(x1_1)\n",
        "    x0_2 = jax.numpy.concatenate([x0_0, x0_1, x0_2], axis=-1)\n",
        "    x0_2 = FlaxDoubleConv(self.nb_filter[0] * 2 + self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])(x0_2, train=train)\n",
        "\n",
        "    x1_2 = FlaxUp(nb_filter=self.nb_filter[1])(x2_1)\n",
        "    x1_2 = jax.numpy.concatenate([x1_0, x1_1, x1_2], axis=-1)\n",
        "    x1_2 = FlaxDoubleConv(self.nb_filter[1] * 2 + self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])(x1_2, train=train)\n",
        "\n",
        "    x2_2 = FlaxUp(nb_filter=self.nb_filter[2])(x3_1)\n",
        "    x2_2 = jax.numpy.concatenate([x2_0, x2_1, x2_2], axis=-1)\n",
        "    x2_2 = FlaxDoubleConv(self.nb_filter[2] * 2 + self.nb_filter[3], self.nb_filter[2], self.nb_filter[2])(x2_2, train=train)\n",
        "\n",
        "    x0_3 = FlaxUp(nb_filter=self.nb_filter[0])(x1_2)\n",
        "    x0_3 = jax.numpy.concatenate([x0_0, x0_1, x0_2, x0_3], axis=-1)\n",
        "    x0_3 = FlaxDoubleConv(self.nb_filter[0] * 3 + self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])(x0_3, train=train)\n",
        "\n",
        "    x1_3 = FlaxUp(nb_filter=self.nb_filter[1])(x2_2)\n",
        "    x1_3 = jax.numpy.concatenate([x1_0, x1_1, x1_2, x1_3], axis=-1)\n",
        "    x1_3 = FlaxDoubleConv(self.nb_filter[1] * 3 + self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])(x1_3, train=train)\n",
        "\n",
        "    x0_4 = FlaxUp(nb_filter=self.nb_filter[0])(x1_3)\n",
        "    x0_4 = jax.numpy.concatenate([x0_0, x0_1, x0_2, x0_3, x0_4], axis=-1)\n",
        "    x0_4 = FlaxDoubleConv(self.nb_filter[0] * 4 + self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])(x0_4, train=train)\n",
        "\n",
        "    # MODIFIED: Classification Head\n",
        "    x = jax.numpy.mean(x0_4, axis=(1, 2))  # Global Average Pooling\n",
        "    x = flax.linen.Dense(256)(x)\n",
        "    x = flax.linen.relu(x)\n",
        "    x = flax.linen.Dropout(rate=HYPERPARAMS[\"dropout\"], deterministic=not train)(x)\n",
        "    x = flax.linen.Dense(self.num_classes)(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "O_Kb0ZQwidhB"
      },
      "outputs": [],
      "source": [
        "class DylanImagePatcher(equinox.Module):\n",
        "  \"\"\"\n",
        "  Converts images into patch embeddings. \n",
        "  Renamed from ImagePatcher to avoid conflict with existing class.\n",
        "  \"\"\"\n",
        "  patch_size: int\n",
        "  d_model: int\n",
        "  proj: equinox.nn.Linear\n",
        "\n",
        "  def __init__(self, patch_size: int, d_model: int, key):\n",
        "    \"\"\"\n",
        "    Initialize DylanImagePatcher.\n",
        "    :param patch_size: Size of each square patch \n",
        "    :param d_model: Embedding dimension \n",
        "    :param key: Random key for initialization \n",
        "    \"\"\"\n",
        "    self.patch_size = patch_size\n",
        "    self.d_model = d_model\n",
        "    # Project flattened patches to d_model  d_model\n",
        "    self.proj = equinox.nn.Linear(patch_size * patch_size * 3, d_model, key=key)\n",
        "\n",
        "  def __call__(self, x: jax.Array) -> jax.Array:\n",
        "    \"\"\"\n",
        "    Forward pass of DylanImagePatcher.\n",
        "    :param x: Input image (H, W, C)  (H, W, C)\n",
        "    :return: Patch embeddings (N, d_model) where N is number of patches  (N, d_model) N \n",
        "    \"\"\"\n",
        "    H, W, C = x.shape\n",
        "    assert H % self.patch_size == 0 and W % self.patch_size == 0, \"Image dimensions must be divisible by patch_size\"\n",
        "\n",
        "    # Reshape into patches: (H, W, C) -> (num_patches_h, num_patches_w, patch_size, patch_size, C)\n",
        "    # (H, W, C) -> (num_patches_h, num_patches_w, patch_size, patch_size, C)\n",
        "    num_patches_h = H // self.patch_size\n",
        "    num_patches_w = W // self.patch_size\n",
        "\n",
        "    x = jax.numpy.reshape(x, (num_patches_h, self.patch_size, num_patches_w, self.patch_size, C))\n",
        "    x = jax.numpy.transpose(x, (0, 2, 1, 3, 4))  # (num_patches_h, num_patches_w, patch_size, patch_size, C)\n",
        "    x = jax.numpy.reshape(x, (num_patches_h * num_patches_w, self.patch_size * self.patch_size * C))\n",
        "\n",
        "    # Project to d_model  d_model\n",
        "    x = jax.vmap(self.proj)(x)\n",
        "    return x\n",
        "\n",
        "  def num_patches(self, image_size: int) -> int:\n",
        "    \"\"\"\n",
        "    Calculate number of patches for a square image.\n",
        "    :param image_size: Size of square image \n",
        "    :return: Number of patches \n",
        "    \"\"\"\n",
        "    return (image_size // self.patch_size) ** 2\n",
        "\n",
        "\n",
        "class SSMBranch(equinox.Module):\n",
        "  \"\"\"\n",
        "  State Space Model (SSM) Branch implementing selective scan mechanism.\n",
        "  SSM\n",
        "  \"\"\"\n",
        "  d_model: int\n",
        "  d_inner: int\n",
        "  d_state: int\n",
        "  dt_rank: int\n",
        "  dt_clip_max: float\n",
        "  exp_clip: float\n",
        "\n",
        "  # Learnable parameters \n",
        "  in_proj: equinox.nn.Linear\n",
        "  dt_proj1: equinox.nn.Linear\n",
        "  dt_proj2: equinox.nn.Linear\n",
        "  B_proj: equinox.nn.Linear\n",
        "  C_proj: equinox.nn.Linear\n",
        "  out_proj: equinox.nn.Linear\n",
        "  A_log: jax.Array\n",
        "  D_param: jax.Array\n",
        "\n",
        "  def __init__(self, d_model: int, d_state: int = 16, dt_rank: int = 16,\n",
        "               expend: int = 2, dt_clip_max: float = 10.0, exp_clip: float = 5.0,\n",
        "               *, key):\n",
        "    \"\"\"\n",
        "    Initialize SSM Branch.\n",
        "    \"\"\"\n",
        "    self.d_model = d_model\n",
        "    self.d_inner = d_model * expend\n",
        "    self.d_state = d_state\n",
        "    self.dt_rank = dt_rank\n",
        "    self.dt_clip_max = dt_clip_max\n",
        "    self.exp_clip = exp_clip\n",
        "\n",
        "    keys = jax.random.split(key, 7)\n",
        "\n",
        "    # Input projection to (d_inner * 2) for x_main and x_gate\n",
        "    #  (d_inner * 2)  x_main  x_gate\n",
        "    self.in_proj = equinox.nn.Linear(d_model, self.d_inner * 2, key=keys[0])\n",
        "\n",
        "    # Time step projection: d_inner -> dt_rank -> d_inner\n",
        "    # d_inner -> dt_rank -> d_inner\n",
        "    self.dt_proj1 = equinox.nn.Linear(self.d_inner, dt_rank, key=keys[1])\n",
        "    self.dt_proj2 = equinox.nn.Linear(dt_rank, self.d_inner, key=keys[2])\n",
        "\n",
        "    # SSM parameter projections SSM \n",
        "    self.B_proj = equinox.nn.Linear(self.d_inner, d_state, use_bias=False, key=keys[3])\n",
        "    self.C_proj = equinox.nn.Linear(self.d_inner, d_state, use_bias=False, key=keys[4])\n",
        "\n",
        "    # Output projection \n",
        "    self.out_proj = equinox.nn.Linear(self.d_inner, d_model, use_bias=False, key=keys[5])\n",
        "\n",
        "    # A parameter: initialized uniform(-5, -1) A  uniform(-5, -1)\n",
        "    self.A_log = jax.random.uniform(keys[6], (self.d_inner, d_state), minval=-5.0, maxval=-1.0)\n",
        "\n",
        "    # D parameter: initialized to 0.1 D  0.1\n",
        "    self.D_param = 0.1 * jax.numpy.ones((self.d_inner,), dtype=jax.numpy.float32)\n",
        "\n",
        "  def __call__(self, x: jax.Array) -> jax.Array:\n",
        "    \"\"\"\n",
        "    Forward pass of SSM Branch with bidirectional scanning.\n",
        "    \"\"\"\n",
        "    L = x.shape[0]\n",
        "\n",
        "    # Initial projection and split \n",
        "    x_proj = jax.vmap(self.in_proj)(x)  # (L, d_inner * 2)\n",
        "    x_main, x_gate = jax.numpy.split(x_proj, 2, axis=-1)  # Each (L, d_inner)  (L, d_inner)\n",
        "    x_main = jax.nn.silu(x_main)  # Activation \n",
        "\n",
        "    # Generate time step dt  dt\n",
        "    dt = jax.vmap(self.dt_proj1)(x_main)  # (L, dt_rank)\n",
        "    dt = jax.vmap(self.dt_proj2)(dt)  # (L, d_inner)\n",
        "    dt = jax.nn.softplus(dt)  # Ensure positivity \n",
        "    dt = jax.numpy.clip(dt, 1e-6, self.dt_clip_max)  # (L, d_inner)\n",
        "\n",
        "    # SSM parameters SSM \n",
        "    A = -jax.numpy.exp(self.A_log.astype(jax.numpy.float32))  # (d_inner, d_state) - ensure negative \n",
        "    B = jax.vmap(self.B_proj)(x_main)  # (L, d_state)\n",
        "    C = jax.vmap(self.C_proj)(x_main)  # (L, d_state)\n",
        "\n",
        "    # Define SSM scan function  SSM \n",
        "    def ssm_scan(u_seq, dt_seq, B_seq, C_seq):\n",
        "      def step_fn(h, inputs):\n",
        "        u_t, dt_t, B_t, C_t = inputs\n",
        "        u_t = u_t.astype(jax.numpy.float32)\n",
        "        dt_t = dt_t.astype(jax.numpy.float32)\n",
        "\n",
        "        # Discretization: A_bar = exp(A * dt) A_bar = exp(A * dt)\n",
        "        dt_expanded = dt_t[:, None]  # (d_inner, 1)\n",
        "        dt_expanded = jax.numpy.clip(dt_expanded, 1e-6, self.dt_clip_max)\n",
        "\n",
        "        exp_arg = A * dt_expanded  # (d_inner, d_state)\n",
        "        exp_arg = jax.numpy.clip(exp_arg, -self.exp_clip, self.exp_clip)\n",
        "        A_bar = jax.numpy.exp(exp_arg)  # (d_inner, d_state)\n",
        "\n",
        "        # B_bar = B * dt B_bar = B * dt\n",
        "        B_bar = B_t[None, :] * dt_expanded  # (d_inner, d_state)\n",
        "\n",
        "        # State update: h_new = A_bar * h + B_bar * u h_new = A_bar * h + B_bar * u\n",
        "        u_expanded = u_t[:, None]  # (d_inner, 1)\n",
        "        h_new = A_bar * h + B_bar * u_expanded  # (d_inner, d_state)\n",
        "\n",
        "        # Output: y = C @ h + D * u y = C @ h + D * u\n",
        "        y_t = jax.numpy.sum(C_t[None, :] * h_new, axis=-1)  # (d_inner,)\n",
        "        y_t = y_t + self.D_param * u_t  # (d_inner,)\n",
        "\n",
        "        return h_new, y_t\n",
        "\n",
        "      h0 = jax.numpy.zeros((self.d_inner, self.d_state), dtype=jax.numpy.float32)\n",
        "      _, y_seq = jax.lax.scan(step_fn, h0, (u_seq, dt_seq, B_seq, C_seq))\n",
        "      return y_seq\n",
        "\n",
        "    # Bidirectional scan \n",
        "    y_forward = ssm_scan(x_main, dt, B, C)  # (L, d_inner)\n",
        "    y_backward = ssm_scan(x_main[::-1], dt[::-1], B[::-1], C[::-1])\n",
        "    y_backward = y_backward[::-1]  # Reverse back\n",
        "\n",
        "    y = y_forward + y_backward  # Combine\n",
        "\n",
        "    # Gate and project \n",
        "    y = y * jax.nn.silu(x_gate)  # (L, d_inner)\n",
        "    y = jax.vmap(self.out_proj)(y)  # (L, d_model)\n",
        "    return y\n",
        "\n",
        "\n",
        "class ConvBranch(equinox.Module):\n",
        "  \"\"\"\n",
        "  Convolutional Branch for Vision Mamba.  Mamba \n",
        "  \"\"\"\n",
        "  proj: equinox.nn.Linear\n",
        "\n",
        "  def __init__(self, d_model: int, *, key):\n",
        "    # Simple projection without actual convolution \n",
        "    self.proj = equinox.nn.Linear(d_model, d_model, use_bias=False, key=key)\n",
        "\n",
        "  def __call__(self, x: jax.Array) -> jax.Array:\n",
        "    x = jax.vmap(self.proj)(x)\n",
        "    x = jax.nn.silu(x)  # Activation \n",
        "    return x\n",
        "\n",
        "\n",
        "class VisionMambaBlock(equinox.Module):\n",
        "  \"\"\"\n",
        "  Vision Mamba Block with Conv and SSM branches.  Conv  SSM  Mamba \n",
        "  \"\"\"\n",
        "  conv_branch: ConvBranch\n",
        "  ssm_branch: SSMBranch\n",
        "  norm: equinox.nn.LayerNorm\n",
        "  combine_proj: equinox.nn.Linear\n",
        "  ffn_norm: equinox.nn.LayerNorm\n",
        "  ffn_proj: equinox.nn.Linear\n",
        "\n",
        "  def __init__(self, d_model: int, d_state: int = 16, dt_rank: int = 16,\n",
        "               ssm_expend: int = 2, *, key):\n",
        "    keys = jax.random.split(key, 5)\n",
        "    self.norm = equinox.nn.LayerNorm(d_model)\n",
        "    self.conv_branch = ConvBranch(d_model, key=keys[0])\n",
        "    self.ssm_branch = SSMBranch(d_model, d_state, dt_rank, ssm_expend, key=keys[1])\n",
        "\n",
        "    # Combine both branches (d_model + d_model -> d_model)\n",
        "    self.combine_proj = equinox.nn.Linear(d_model * 2, d_model, use_bias=False, key=keys[2])\n",
        "\n",
        "    # SwiGLU FFN SwiGLU \n",
        "    self.ffn_norm = equinox.nn.LayerNorm(d_model)\n",
        "    self.ffn_proj = equinox.nn.Linear(d_model, d_model * 2, key=keys[3])\n",
        "\n",
        "  def __call__(self, x: jax.Array) -> jax.Array:\n",
        "    x_norm = jax.vmap(self.norm)(x)\n",
        "\n",
        "    # Parallel branches \n",
        "    conv_out = self.conv_branch(x_norm)\n",
        "    ssm_out = self.ssm_branch(x_norm)\n",
        "\n",
        "    # Combine\n",
        "    combined = jax.numpy.concatenate([conv_out, ssm_out], axis=-1)\n",
        "    combined = jax.vmap(self.combine_proj)(combined)\n",
        "    x = x + combined  # Residual 1\n",
        "\n",
        "    # SwiGLU FFN\n",
        "    x_norm = jax.vmap(self.ffn_norm)(x)\n",
        "    ff = jax.vmap(self.ffn_proj)(x_norm)\n",
        "    f1, f2 = jax.numpy.split(ff, 2, axis=-1)\n",
        "    f = jax.nn.silu(f2) * f1\n",
        "    x = x + f  # Residual 2\n",
        "    return x\n",
        "\n",
        "\n",
        "class DylanMamba(equinox.Module):\n",
        "  \"\"\"\n",
        "  DylanMamba: Bidirectional Vision Mamba for classification.\n",
        "  DylanMamba Mamba\n",
        "  \"\"\"\n",
        "  patcher: DylanImagePatcher\n",
        "  blocks: list\n",
        "  norm: equinox.nn.LayerNorm\n",
        "  dropout: equinox.nn.Dropout\n",
        "  classifier: equinox.nn.Linear\n",
        "  pos_embed: jax.Array\n",
        "\n",
        "  def __init__(self, patch_size: int, num_layers: int, d_model: int, d_state: int,\n",
        "               num_classes: int, ssm_expend: int = 2, dt_rank: int = 16,\n",
        "               *, key, image_size: int):\n",
        "    \"\"\"\n",
        "    Initialize DylanMamba.\n",
        "    \"\"\"\n",
        "    assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n",
        "    keys = jax.random.split(key, num_layers + 4)\n",
        "\n",
        "    self.patcher = DylanImagePatcher(patch_size, d_model, keys[0])\n",
        "    self.blocks = [\n",
        "      VisionMambaBlock(d_model, d_state, dt_rank, ssm_expend, key=keys[i+1])\n",
        "      for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    self.norm = equinox.nn.LayerNorm(d_model)\n",
        "    self.dropout = equinox.nn.Dropout(p=HYPERPARAMS[\"dropout\"])\n",
        "    self.classifier = equinox.nn.Linear(d_model, num_classes, key=keys[-1])\n",
        "\n",
        "    num_patches = self.patcher.num_patches(image_size)\n",
        "    self.pos_embed = jax.random.normal(keys[-2], (num_patches, d_model)) * 0.02\n",
        "\n",
        "  def __call__(self, x: jax.Array, key=None, train: bool = True) -> jax.Array:\n",
        "    \"\"\"\n",
        "    Forward pass of DylanMamba.\n",
        "    \"\"\"\n",
        "    x = self.patcher(x)  # (N, d_model)\n",
        "    x = x + self.pos_embed  # Add positional embeddings\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    x = jax.vmap(self.norm)(x)\n",
        "    x = jax.numpy.mean(x, axis=0)  # Global pooling\n",
        "\n",
        "    if key is None:\n",
        "      key = jax.random.PRNGKey(0)\n",
        "    x = self.dropout(x, key=key, inference=not train)\n",
        "    logits = self.classifier(x)\n",
        "    return logits\n",
        "\n",
        "  def num_patches(self, image_size: int) -> int:\n",
        "    return self.patcher.num_patches(image_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-rw9NaMZA41"
      },
      "source": [
        "# setup for trainin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "y5pAVlqzGqa9"
      },
      "outputs": [],
      "source": [
        "def save_model_params(state,filename):\n",
        "  # ensure saving happens under BASE_DIR if relative path provided\n",
        "  filepath = filename if os.path.isabs(filename) else bpath(filename)\n",
        "  if hasattr(state,'params'):\n",
        "    with open(filepath, 'wb') as file:\n",
        "      file.write(serialization.to_bytes(state.params))\n",
        "  else:\n",
        "    equinox.tree_serialise_leaves(filepath,state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UX-nEmlbMkQq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1205 22:19:27.086781     645 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
            "W1205 22:19:27.094127     493 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(HYPERPARAMS['seed'])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  train_datast,\n",
        "  batch_size=HYPERPARAMS['batch_size'],\n",
        "  shuffle=True,\n",
        "  num_workers = 0,\n",
        "  drop_last=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "  val_dataset,\n",
        "  batch_size=HYPERPARAMS['batch_size'],\n",
        "  shuffle=False,\n",
        "  num_workers = 0,\n",
        "  drop_last = True\n",
        ")\n",
        "mamba_train_loader = train_loader\n",
        "mamba_val_loader = val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZONvsUtWmpad"
      },
      "source": [
        "## TIME TO TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHEBMvJvRes"
      },
      "source": [
        "cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_djEcb83vKIt",
        "outputId": "6eead564-cae8-41c0-e498-6a2a74ae82b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch1 avgtrainingloss2.2429 avgtrainingaccuracy0.1705 avgvalloss2.1140 avgvalaccuracy0.2228 time35.3\n",
            "epoch2 avgtrainingloss2.1340 avgtrainingaccuracy0.2037 avgvalloss1.9714 avgvalaccuracy0.2772 time19.8\n",
            "epoch3 avgtrainingloss2.0361 avgtrainingaccuracy0.2402 avgvalloss1.8553 avgvalaccuracy0.3163 time18.1\n",
            "epoch4 avgtrainingloss1.9259 avgtrainingaccuracy0.2751 avgvalloss1.7849 avgvalaccuracy0.4167 time17.7\n",
            "epoch5 avgtrainingloss1.8791 avgtrainingaccuracy0.3040 avgvalloss1.6676 avgvalaccuracy0.3810 time21.0\n",
            "epoch6 avgtrainingloss1.8083 avgtrainingaccuracy0.3393 avgvalloss1.6826 avgvalaccuracy0.4439 time17.7\n",
            "epoch7 avgtrainingloss1.7579 avgtrainingaccuracy0.3601 avgvalloss1.5607 avgvalaccuracy0.4439 time64.3\n",
            "epoch8 avgtrainingloss1.6946 avgtrainingaccuracy0.3801 avgvalloss1.5179 avgvalaccuracy0.4762 time26.8\n",
            "epoch9 avgtrainingloss1.6120 avgtrainingaccuracy0.4124 avgvalloss1.4850 avgvalaccuracy0.4949 time23.0\n",
            "epoch10 avgtrainingloss1.5945 avgtrainingaccuracy0.4239 avgvalloss1.4646 avgvalaccuracy0.4609 time25.1\n",
            "epoch11 avgtrainingloss1.5407 avgtrainingaccuracy0.4532 avgvalloss1.4061 avgvalaccuracy0.5102 time25.0\n",
            "epoch12 avgtrainingloss1.4903 avgtrainingaccuracy0.4643 avgvalloss1.3997 avgvalaccuracy0.5051 time24.9\n",
            "epoch13 avgtrainingloss1.4321 avgtrainingaccuracy0.4881 avgvalloss1.3887 avgvalaccuracy0.5323 time21.9\n",
            "epoch14 avgtrainingloss1.3771 avgtrainingaccuracy0.5208 avgvalloss1.3760 avgvalaccuracy0.5272 time25.0\n",
            "epoch15 avgtrainingloss1.3545 avgtrainingaccuracy0.5251 avgvalloss1.2929 avgvalaccuracy0.5527 time25.4\n",
            "epoch16 avgtrainingloss1.2845 avgtrainingaccuracy0.5429 avgvalloss1.3072 avgvalaccuracy0.5850 time21.9\n",
            "epoch17 avgtrainingloss1.2814 avgtrainingaccuracy0.5587 avgvalloss1.2576 avgvalaccuracy0.5731 time24.4\n",
            "epoch18 avgtrainingloss1.2263 avgtrainingaccuracy0.5821 avgvalloss1.2459 avgvalaccuracy0.6020 time24.6\n",
            "epoch19 avgtrainingloss1.1558 avgtrainingaccuracy0.5974 avgvalloss1.2035 avgvalaccuracy0.6122 time21.5\n",
            "epoch20 avgtrainingloss1.1440 avgtrainingaccuracy0.5957 avgvalloss1.2724 avgvalaccuracy0.5765 time24.8\n",
            "epoch21 avgtrainingloss1.1284 avgtrainingaccuracy0.6059 avgvalloss1.1752 avgvalaccuracy0.6241 time24.0\n",
            "epoch22 avgtrainingloss1.0407 avgtrainingaccuracy0.6305 avgvalloss1.1770 avgvalaccuracy0.6088 time22.1\n",
            "epoch23 avgtrainingloss1.0391 avgtrainingaccuracy0.6467 avgvalloss1.1444 avgvalaccuracy0.6310 time25.5\n",
            "epoch24 avgtrainingloss1.0150 avgtrainingaccuracy0.6446 avgvalloss1.1513 avgvalaccuracy0.6173 time24.9\n",
            "epoch25 avgtrainingloss0.9645 avgtrainingaccuracy0.6620 avgvalloss1.1217 avgvalaccuracy0.6361 time22.5\n",
            "epoch26 avgtrainingloss0.9231 avgtrainingaccuracy0.6764 avgvalloss1.1535 avgvalaccuracy0.6480 time24.1\n",
            "epoch27 avgtrainingloss0.9170 avgtrainingaccuracy0.6726 avgvalloss1.1319 avgvalaccuracy0.6344 time22.9\n",
            "epoch28 avgtrainingloss0.9325 avgtrainingaccuracy0.6739 avgvalloss1.0984 avgvalaccuracy0.6548 time24.6\n",
            "epoch29 avgtrainingloss0.8937 avgtrainingaccuracy0.6837 avgvalloss1.1295 avgvalaccuracy0.6497 time23.8\n",
            "epoch30 avgtrainingloss0.8527 avgtrainingaccuracy0.7028 avgvalloss1.1626 avgvalaccuracy0.6531 time20.7\n",
            "743.3322975635529\n"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(HYPERPARAMS[\"seed\"])\n",
        "k1, k2 = jax.random.split(key, 2)\n",
        "baseline_model = Baseline(\n",
        "  image_size=HYPERPARAMS[\"image_size\"],\n",
        "  in_ch=3,\n",
        "  num_classes=HYPERPARAMS[\"num_classes\"],\n",
        "  key=k1,\n",
        ")\n",
        "baseline_trained, baseline_history = JAXDataLoader.train_model(baseline_model, train_loader, val_loader, HYPERPARAMS, \"Baseline CNN\")\n",
        "save_model_params(baseline_trained, 'baseline_params.eqx')\n",
        "try:\n",
        "  del baseline_trained, baseline_model\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()\n",
        "jax.clear_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoMSBZIPtkqA"
      },
      "source": [
        "one direction mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52pZfNkdmrSv",
        "outputId": "9319550c-5768-4eef-ebd9-c51f7a92df3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch1 avgtrainingloss2.1651 avgtrainingaccuracy0.1986 avgvalloss2.0163 avgvalaccuracy0.2670 time352.9\n",
            "epoch2 avgtrainingloss2.0240 avgtrainingaccuracy0.2479 avgvalloss1.9101 avgvalaccuracy0.3180 time344.5\n",
            "epoch3 avgtrainingloss1.9857 avgtrainingaccuracy0.2887 avgvalloss1.8688 avgvalaccuracy0.3384 time335.2\n",
            "epoch4 avgtrainingloss1.9174 avgtrainingaccuracy0.3185 avgvalloss1.8059 avgvalaccuracy0.3963 time326.8\n",
            "epoch5 avgtrainingloss1.8943 avgtrainingaccuracy0.3274 avgvalloss1.8002 avgvalaccuracy0.3656 time329.0\n",
            "epoch6 avgtrainingloss1.8572 avgtrainingaccuracy0.3516 avgvalloss1.8037 avgvalaccuracy0.3793 time327.0\n",
            "epoch7 avgtrainingloss1.8248 avgtrainingaccuracy0.3605 avgvalloss1.7333 avgvalaccuracy0.3844 time326.8\n",
            "epoch8 avgtrainingloss1.8166 avgtrainingaccuracy0.3452 avgvalloss1.7644 avgvalaccuracy0.3827 time329.5\n",
            "epoch9 avgtrainingloss1.8099 avgtrainingaccuracy0.3618 avgvalloss1.7275 avgvalaccuracy0.3980 time319.7\n",
            "epoch10 avgtrainingloss1.7871 avgtrainingaccuracy0.3652 avgvalloss1.7359 avgvalaccuracy0.4048 time291.2\n",
            "epoch11 avgtrainingloss1.7752 avgtrainingaccuracy0.3793 avgvalloss1.7081 avgvalaccuracy0.3997 time291.4\n",
            "epoch12 avgtrainingloss1.7442 avgtrainingaccuracy0.3856 avgvalloss1.6764 avgvalaccuracy0.4354 time290.7\n",
            "epoch13 avgtrainingloss1.7220 avgtrainingaccuracy0.3912 avgvalloss1.7572 avgvalaccuracy0.3469 time325.3\n",
            "epoch14 avgtrainingloss1.7109 avgtrainingaccuracy0.3958 avgvalloss1.6740 avgvalaccuracy0.4048 time325.4\n",
            "epoch15 avgtrainingloss1.6904 avgtrainingaccuracy0.4001 avgvalloss1.6533 avgvalaccuracy0.4116 time328.6\n",
            "epoch16 avgtrainingloss1.6578 avgtrainingaccuracy0.4069 avgvalloss1.6860 avgvalaccuracy0.3912 time330.0\n",
            "epoch17 avgtrainingloss1.6535 avgtrainingaccuracy0.4286 avgvalloss1.6190 avgvalaccuracy0.4473 time332.3\n",
            "epoch18 avgtrainingloss1.6481 avgtrainingaccuracy0.4201 avgvalloss1.6040 avgvalaccuracy0.4184 time332.1\n",
            "epoch19 avgtrainingloss1.6152 avgtrainingaccuracy0.4273 avgvalloss1.5803 avgvalaccuracy0.4473 time333.1\n",
            "epoch20 avgtrainingloss1.6071 avgtrainingaccuracy0.4371 avgvalloss1.5664 avgvalaccuracy0.4660 time339.0\n",
            "epoch21 avgtrainingloss1.5865 avgtrainingaccuracy0.4524 avgvalloss1.5706 avgvalaccuracy0.4575 time332.0\n",
            "epoch22 avgtrainingloss1.5376 avgtrainingaccuracy0.4588 avgvalloss1.5323 avgvalaccuracy0.4796 time334.9\n",
            "epoch23 avgtrainingloss1.5380 avgtrainingaccuracy0.4600 avgvalloss1.5878 avgvalaccuracy0.4456 time334.2\n",
            "epoch24 avgtrainingloss1.5194 avgtrainingaccuracy0.4668 avgvalloss1.5373 avgvalaccuracy0.4694 time335.3\n",
            "epoch25 avgtrainingloss1.4871 avgtrainingaccuracy0.4766 avgvalloss1.5721 avgvalaccuracy0.4575 time329.5\n",
            "epoch26 avgtrainingloss1.4814 avgtrainingaccuracy0.4813 avgvalloss1.4943 avgvalaccuracy0.4915 time331.9\n",
            "epoch27 avgtrainingloss1.4511 avgtrainingaccuracy0.4783 avgvalloss1.4911 avgvalaccuracy0.4881 time334.0\n",
            "epoch28 avgtrainingloss1.4399 avgtrainingaccuracy0.4928 avgvalloss1.4462 avgvalaccuracy0.4915 time329.4\n",
            "epoch29 avgtrainingloss1.4190 avgtrainingaccuracy0.5038 avgvalloss1.4282 avgvalaccuracy0.4949 time335.3\n",
            "epoch30 avgtrainingloss1.3992 avgtrainingaccuracy0.5068 avgvalloss1.4497 avgvalaccuracy0.4796 time332.8\n",
            "9839.676833868027\n"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(HYPERPARAMS['seed'])\n",
        "_,_,k3 = jax.random.split(key,3)\n",
        "unimamba_model = UnidirectionalMamba(\n",
        "  patch_size=HYPERPARAMS['mamba_patch_size'],\n",
        "  num_layers=HYPERPARAMS['num_mamba_layers'],\n",
        "  d_model=HYPERPARAMS['d_model'],\n",
        "  d_state=HYPERPARAMS['d_state'],\n",
        "  num_classes=HYPERPARAMS['num_classes'],\n",
        "  key=k3,\n",
        "  image_size=HYPERPARAMS['image_size']\n",
        ")\n",
        "unimamba_trained, unimamba_history = JAXDataLoader.train_model(unimamba_model,mamba_train_loader,mamba_val_loader,HYPERPARAMS, \"Unidirectional Vision Mamba\")\n",
        "save_model_params(unimamba_trained, \"unimamba_params.eqx\")\n",
        "try:\n",
        "  del unimamba_trained, unimamba_model\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()\n",
        "jax.clear_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BHsSmyIr7zq"
      },
      "source": [
        "both direction mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArB-62Hkmrr-",
        "outputId": "22097097-1863-4d9c-c2e2-9a5896781450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch1 avgtrainingloss2.1679 avgtrainingaccuracy0.2096 avgvalloss2.0040 avgvalaccuracy0.2500 time648.2\n",
            "epoch2 avgtrainingloss2.0543 avgtrainingaccuracy0.2440 avgvalloss1.9677 avgvalaccuracy0.3061 time633.6\n",
            "epoch3 avgtrainingloss1.9892 avgtrainingaccuracy0.2836 avgvalloss1.8626 avgvalaccuracy0.3367 time629.9\n",
            "epoch4 avgtrainingloss1.9107 avgtrainingaccuracy0.3078 avgvalloss1.8524 avgvalaccuracy0.3554 time633.1\n",
            "epoch5 avgtrainingloss1.8904 avgtrainingaccuracy0.3278 avgvalloss1.7869 avgvalaccuracy0.3588 time630.9\n",
            "epoch6 avgtrainingloss1.8451 avgtrainingaccuracy0.3291 avgvalloss1.8115 avgvalaccuracy0.3741 time635.9\n",
            "epoch7 avgtrainingloss1.8054 avgtrainingaccuracy0.3571 avgvalloss1.8722 avgvalaccuracy0.3333 time628.6\n",
            "epoch8 avgtrainingloss1.8015 avgtrainingaccuracy0.3567 avgvalloss1.7157 avgvalaccuracy0.4082 time637.2\n",
            "epoch9 avgtrainingloss1.7723 avgtrainingaccuracy0.3724 avgvalloss1.6962 avgvalaccuracy0.4116 time632.9\n",
            "epoch10 avgtrainingloss1.7601 avgtrainingaccuracy0.3831 avgvalloss1.7128 avgvalaccuracy0.4167 time635.1\n",
            "epoch11 avgtrainingloss1.7444 avgtrainingaccuracy0.3810 avgvalloss1.6735 avgvalaccuracy0.4422 time626.9\n",
            "epoch12 avgtrainingloss1.7041 avgtrainingaccuracy0.4077 avgvalloss1.6647 avgvalaccuracy0.4133 time633.9\n",
            "epoch13 avgtrainingloss1.6926 avgtrainingaccuracy0.3958 avgvalloss1.6279 avgvalaccuracy0.4456 time629.2\n",
            "epoch14 avgtrainingloss1.6663 avgtrainingaccuracy0.4332 avgvalloss1.6039 avgvalaccuracy0.4609 time643.0\n",
            "epoch15 avgtrainingloss1.6393 avgtrainingaccuracy0.4332 avgvalloss1.6080 avgvalaccuracy0.4405 time636.0\n",
            "epoch16 avgtrainingloss1.6148 avgtrainingaccuracy0.4315 avgvalloss1.6121 avgvalaccuracy0.4507 time645.6\n",
            "epoch17 avgtrainingloss1.5731 avgtrainingaccuracy0.4579 avgvalloss1.5699 avgvalaccuracy0.4541 time628.5\n",
            "epoch18 avgtrainingloss1.5381 avgtrainingaccuracy0.4639 avgvalloss1.5454 avgvalaccuracy0.4728 time645.7\n",
            "epoch19 avgtrainingloss1.5269 avgtrainingaccuracy0.4787 avgvalloss1.5569 avgvalaccuracy0.4796 time638.1\n",
            "epoch20 avgtrainingloss1.4998 avgtrainingaccuracy0.4690 avgvalloss1.6009 avgvalaccuracy0.4541 time633.9\n",
            "epoch21 avgtrainingloss1.4607 avgtrainingaccuracy0.4877 avgvalloss1.4732 avgvalaccuracy0.4915 time637.6\n",
            "epoch22 avgtrainingloss1.4310 avgtrainingaccuracy0.5004 avgvalloss1.4618 avgvalaccuracy0.4949 time636.4\n",
            "epoch23 avgtrainingloss1.4051 avgtrainingaccuracy0.5140 avgvalloss1.4721 avgvalaccuracy0.5187 time636.7\n",
            "epoch24 avgtrainingloss1.3878 avgtrainingaccuracy0.5174 avgvalloss1.4325 avgvalaccuracy0.5085 time631.5\n",
            "epoch25 avgtrainingloss1.3335 avgtrainingaccuracy0.5421 avgvalloss1.3797 avgvalaccuracy0.5153 time642.5\n",
            "epoch26 avgtrainingloss1.3401 avgtrainingaccuracy0.5366 avgvalloss1.3695 avgvalaccuracy0.5187 time632.6\n",
            "epoch27 avgtrainingloss1.2970 avgtrainingaccuracy0.5502 avgvalloss1.3111 avgvalaccuracy0.5408 time633.8\n",
            "epoch28 avgtrainingloss1.2495 avgtrainingaccuracy0.5736 avgvalloss1.3272 avgvalaccuracy0.5493 time634.3\n",
            "epoch29 avgtrainingloss1.2192 avgtrainingaccuracy0.5774 avgvalloss1.2931 avgvalaccuracy0.5459 time637.7\n",
            "epoch30 avgtrainingloss1.1791 avgtrainingaccuracy0.5952 avgvalloss1.2775 avgvalaccuracy0.5544 time641.9\n",
            "19071.34059381485\n"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(HYPERPARAMS['seed'])\n",
        "_,_,_,k4 = jax.random.split(key,4)\n",
        "bimamba_model = BidirectionalMamba(\n",
        "  patch_size=HYPERPARAMS.get(\"mamba_patch_size\"),\n",
        "  num_layers=HYPERPARAMS.get(\"num_mamba_layers\"),\n",
        "  d_model=HYPERPARAMS[\"d_model\"],\n",
        "  d_state=HYPERPARAMS[\"d_state\"],\n",
        "  num_classes=HYPERPARAMS[\"num_classes\"],\n",
        "  key=k4,\n",
        "  image_size=HYPERPARAMS[\"image_size\"]\n",
        ")\n",
        "bimamba_trained, bimamba_history = JAXDataLoader.train_model(\n",
        "  bimamba_model,\n",
        "  mamba_train_loader,\n",
        "  mamba_val_loader,\n",
        "  HYPERPARAMS,\n",
        "  \"Bidirectional Vision Mamba (The ViM architecture)\"\n",
        ")\n",
        "save_model_params(bimamba_trained, \"bimamba_params.eqx\")\n",
        "try:\n",
        "  del bimamba_trained, bimamba_model\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()\n",
        "jax.clear_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo98OlaauuoL"
      },
      "source": [
        "dylan mamba, which has a cnn plus mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE20ke-Vutob",
        "outputId": "e57c740a-be49-4784-db7b-dbafbb2f88c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch1 avgtrainingloss2.2465 avgtrainingaccuracy0.1773 avgvalloss2.1656 avgvalaccuracy0.1565 time257.9\n",
            "epoch2 avgtrainingloss2.1090 avgtrainingaccuracy0.2232 avgvalloss2.0427 avgvalaccuracy0.2109 time235.0\n",
            "epoch3 avgtrainingloss2.0414 avgtrainingaccuracy0.2543 avgvalloss1.9318 avgvalaccuracy0.2874 time231.7\n",
            "epoch4 avgtrainingloss1.9709 avgtrainingaccuracy0.2764 avgvalloss1.8503 avgvalaccuracy0.3350 time233.0\n",
            "epoch5 avgtrainingloss1.9195 avgtrainingaccuracy0.2968 avgvalloss1.8228 avgvalaccuracy0.3163 time236.6\n",
            "epoch6 avgtrainingloss1.8655 avgtrainingaccuracy0.3163 avgvalloss1.7075 avgvalaccuracy0.4014 time233.9\n",
            "epoch7 avgtrainingloss1.8161 avgtrainingaccuracy0.3397 avgvalloss1.6536 avgvalaccuracy0.4201 time235.1\n",
            "epoch8 avgtrainingloss1.7729 avgtrainingaccuracy0.3669 avgvalloss1.6667 avgvalaccuracy0.4150 time233.4\n",
            "epoch9 avgtrainingloss1.7113 avgtrainingaccuracy0.3992 avgvalloss1.6874 avgvalaccuracy0.3980 time230.9\n",
            "epoch10 avgtrainingloss1.6629 avgtrainingaccuracy0.4043 avgvalloss1.5645 avgvalaccuracy0.4592 time235.4\n",
            "epoch11 avgtrainingloss1.6204 avgtrainingaccuracy0.4205 avgvalloss1.5836 avgvalaccuracy0.4643 time235.4\n",
            "epoch12 avgtrainingloss1.5731 avgtrainingaccuracy0.4511 avgvalloss1.5127 avgvalaccuracy0.4660 time232.3\n",
            "epoch13 avgtrainingloss1.5302 avgtrainingaccuracy0.4664 avgvalloss1.4469 avgvalaccuracy0.4983 time233.2\n",
            "epoch14 avgtrainingloss1.4771 avgtrainingaccuracy0.4860 avgvalloss1.6406 avgvalaccuracy0.4422 time235.4\n",
            "epoch15 avgtrainingloss1.4300 avgtrainingaccuracy0.4966 avgvalloss1.4141 avgvalaccuracy0.4949 time232.9\n",
            "epoch16 avgtrainingloss1.4053 avgtrainingaccuracy0.4940 avgvalloss1.3628 avgvalaccuracy0.5136 time232.4\n",
            "epoch17 avgtrainingloss1.3565 avgtrainingaccuracy0.5276 avgvalloss1.2931 avgvalaccuracy0.5476 time236.5\n",
            "epoch18 avgtrainingloss1.3108 avgtrainingaccuracy0.5412 avgvalloss1.3131 avgvalaccuracy0.5425 time232.0\n",
            "epoch19 avgtrainingloss1.2806 avgtrainingaccuracy0.5561 avgvalloss1.2963 avgvalaccuracy0.5527 time229.9\n",
            "epoch20 avgtrainingloss1.2340 avgtrainingaccuracy0.5697 avgvalloss1.3218 avgvalaccuracy0.5425 time231.3\n",
            "epoch21 avgtrainingloss1.2041 avgtrainingaccuracy0.5812 avgvalloss1.2845 avgvalaccuracy0.5731 time232.3\n",
            "epoch22 avgtrainingloss1.1748 avgtrainingaccuracy0.5906 avgvalloss1.2957 avgvalaccuracy0.5544 time229.6\n",
            "epoch23 avgtrainingloss1.1278 avgtrainingaccuracy0.6071 avgvalloss1.1987 avgvalaccuracy0.6054 time231.1\n",
            "epoch24 avgtrainingloss1.0774 avgtrainingaccuracy0.6263 avgvalloss1.1877 avgvalaccuracy0.6020 time227.6\n",
            "epoch25 avgtrainingloss1.0570 avgtrainingaccuracy0.6509 avgvalloss1.2266 avgvalaccuracy0.6105 time234.9\n",
            "epoch26 avgtrainingloss0.9939 avgtrainingaccuracy0.6637 avgvalloss1.1430 avgvalaccuracy0.6122 time234.2\n",
            "epoch27 avgtrainingloss0.9541 avgtrainingaccuracy0.6684 avgvalloss1.2245 avgvalaccuracy0.6037 time231.5\n",
            "epoch28 avgtrainingloss0.9164 avgtrainingaccuracy0.6879 avgvalloss1.1959 avgvalaccuracy0.6259 time228.3\n",
            "epoch29 avgtrainingloss0.9118 avgtrainingaccuracy0.6884 avgvalloss1.1321 avgvalaccuracy0.6259 time232.3\n",
            "epoch30 avgtrainingloss0.8365 avgtrainingaccuracy0.7173 avgvalloss1.1865 avgvalaccuracy0.5952 time234.4\n",
            "7010.429266214371\n"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(HYPERPARAMS[\"seed\"])\n",
        "_, _, k_dylan = jax.random.split(key, 3)\n",
        "dylan_model = DylanMamba(\n",
        "  patch_size=HYPERPARAMS.get(\"dylan_patch_size\", 16),\n",
        "  num_layers=HYPERPARAMS.get(\"num_dylan_layers\", 4),\n",
        "  d_model=HYPERPARAMS[\"d_model\"],\n",
        "  d_state=HYPERPARAMS[\"d_state\"],\n",
        "  num_classes=HYPERPARAMS[\"num_classes\"],\n",
        "  ssm_expend=HYPERPARAMS.get(\"dylan_ssm_expend\", 2),\n",
        "  dt_rank=HYPERPARAMS.get(\"dylan_dt_rank\", 16),\n",
        "  key=k_dylan,\n",
        "  image_size=HYPERPARAMS[\"image_size\"],\n",
        ")\n",
        "dylan_trained, dylan_history = JAXDataLoader.train_model(dylan_model, train_loader, val_loader, HYPERPARAMS, \"DylanMamba\")\n",
        "save_model_params(dylan_trained, 'dylan_mamba_params.eqx')\n",
        "try:\n",
        "  del dylan_trained, dylan_model\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()\n",
        "jax.clear_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Twrbtuu3yw"
      },
      "source": [
        "unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfVBq_NcmrVE",
        "outputId": "a9056bab-9597-468b-e458-4fe3d1db793d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1206 08:30:53.723242     674 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.04GiB (6487217930 bytes) by rematerialization; only reduced to 8.15GiB (8751022128 bytes), down from 8.15GiB (8751022128 bytes) originally\n",
            "W1206 08:31:03.813610     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.14GiB (rounded to 8742634240)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:31:03.813953     493 bfc_allocator.cc:512] **__________________________________________________________________________________________________\n",
            "W1206 08:31:05.710443     657 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.04GiB (6489311806 bytes) by rematerialization; only reduced to 8.17GiB (8775532592 bytes), down from 8.17GiB (8775532592 bytes) originally\n",
            "W1206 08:31:16.133960     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.17GiB (rounded to 8771339008)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:31:16.134086     493 bfc_allocator.cc:512] **__________________________________________________________________________________________________\n",
            "W1206 08:31:18.169273     657 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.04GiB (6484475249 bytes) by rematerialization; only reduced to 8.29GiB (8900837424 bytes), down from 8.29GiB (8900837424 bytes) originally\n",
            "W1206 08:31:28.232309     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.29GiB (rounded to 8898740992)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:31:28.232517     493 bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
            "W1206 08:31:30.087074     674 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.01GiB (6458522993 bytes) by rematerialization; only reduced to 8.58GiB (9214361648 bytes), down from 8.58GiB (9214361648 bytes) originally\n",
            "W1206 08:31:40.186474     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.58GiB (rounded to 9213313792)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:31:40.186653     493 bfc_allocator.cc:512] ****________________________________________________________________________________________________\n",
            "W1206 08:31:41.435680     660 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.02GiB (6462560010 bytes) by rematerialization; only reduced to 24.86GiB (26694123568 bytes), down from 24.86GiB (26694123568 bytes) originally\n",
            "W1206 08:31:51.558855     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 24.86GiB (rounded to 26692027136)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:31:51.559088     493 bfc_allocator.cc:512] *****_______________________________________________________________________________________________\n",
            "W1206 08:31:52.854145     668 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.03GiB (6476859966 bytes) by rematerialization; only reduced to 24.50GiB (26309820464 bytes), down from 24.50GiB (26309820464 bytes) originally\n",
            "W1206 08:32:02.974299     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 24.50GiB (rounded to 26305626880)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:32:02.974544     493 bfc_allocator.cc:512] *****_______________________________________________________________________________________________\n",
            "W1206 08:32:04.205936     662 hlo_rematerialization.cc:3204] Can't reduce memory use below 6.03GiB (6470158910 bytes) by rematerialization; only reduced to 24.42GiB (26219511856 bytes), down from 24.42GiB (26219511856 bytes) originally\n",
            "W1206 08:32:14.300286     493 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 24.41GiB (rounded to 26211123968)requested by op \n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "W1206 08:32:14.300535     493 bfc_allocator.cc:512] ******______________________________________________________________________________________________\n",
            "E1206 08:32:20.375409     493 cuda_executor.cc:271] failed to unload module 0x46960410; leaking: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\n",
            "E1206 08:32:20.375551     493 cuda_executor.cc:271] failed to unload module 0x3e6da380; leaking: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\n",
            "E1206 08:32:20.375600     493 cuda_executor.cc:271] failed to unload module 0x462a7770; leaking: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\n",
            "E1206 08:32:20.385381     493 cuda_executor.cc:1599] failed to query device memory info: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\n",
            "E1206 08:32:20.385656     493 se_gpu_pjrt_client.cc:897] Failed to query available memory for GPU 0\n"
          ]
        },
        {
          "ename": "JaxRuntimeError",
          "evalue": "INTERNAL: Autotuner could not find any supported configs for HLO: %custom-call.135 = (f32[65536,32]{1,0}, s8[16785408]{0}) custom-call(%bitcast.121, %select_n.58), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/transpose(jvp(FlaxUNet))/Up_1/jit(_resize)/dot_general\" source_file=\"/tmp/ipykernel_493/945805572.py\" source_line=32 source_end_line=32 source_column=9 source_end_column=108}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"],\"algorithm\":\"ALG_UNSET\"},\"epilogue\":\"DEFAULT\",\"lhs_stride\":\"4194304\",\"rhs_stride\":\"2048\",\"grad_x\":false,\"grad_y\":false,\"damax_output\":false},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_INVALID\"}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mJaxRuntimeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m flax_unet_state, flax_unet_history = \u001b[43mtrain_flax_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFlaxUNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHYPERPARAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFlax UNet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m save_model_params(flax_unet_state, \u001b[33m\"\u001b[39m\u001b[33mflax_unet_params.eqx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mtrain_flax_model\u001b[39m\u001b[34m(model_class, train_loader, val_loader, params, model_name)\u001b[39m\n\u001b[32m     85\u001b[39m images, labels = convert_equinox_to_flax_batch(batch)\n\u001b[32m     86\u001b[39m rng, dropout_rng = jax.random.split(rng)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m state, batch_stats, loss, acc = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m train_loss += loss\n\u001b[32m     89\u001b[39m train_acc += acc\n",
            "    \u001b[31m[... skipping hidden 11 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jax_env/lib/python3.12/site-packages/jax/_src/compiler.py:362\u001b[39m, in \u001b[36mbackend_compile_and_load\u001b[39m\u001b[34m(backend, module, executable_devices, options, host_callbacks)\u001b[39m\n\u001b[32m    353\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile_and_load(\n\u001b[32m    354\u001b[39m           module,\n\u001b[32m    355\u001b[39m           executable_devices=executable_devices,\n\u001b[32m    356\u001b[39m           compile_options=options,\n\u001b[32m    357\u001b[39m           host_callbacks=host_callbacks,\n\u001b[32m    358\u001b[39m       )\n\u001b[32m    359\u001b[39m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    360\u001b[39m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _jax.JaxRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    368\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
            "\u001b[31mJaxRuntimeError\u001b[39m: INTERNAL: Autotuner could not find any supported configs for HLO: %custom-call.135 = (f32[65536,32]{1,0}, s8[16785408]{0}) custom-call(%bitcast.121, %select_n.58), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/transpose(jvp(FlaxUNet))/Up_1/jit(_resize)/dot_general\" source_file=\"/tmp/ipykernel_493/945805572.py\" source_line=32 source_end_line=32 source_column=9 source_end_column=108}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"],\"algorithm\":\"ALG_UNSET\"},\"epilogue\":\"DEFAULT\",\"lhs_stride\":\"4194304\",\"rhs_stride\":\"2048\",\"grad_x\":false,\"grad_y\":false,\"damax_output\":false},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_INVALID\"}"
          ]
        }
      ],
      "source": [
        "flax_unet_state, flax_unet_history = train_flax_model(FlaxUNet, train_loader, val_loader, HYPERPARAMS, \"Flax UNet\")\n",
        "save_model_params(flax_unet_state, \"flax_unet_params.eqx\")\n",
        "try:\n",
        "  del flax_unet_state\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()\n",
        "jax.clear_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gDu92qRu4hd"
      },
      "source": [
        "nested unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGHscGFrmrXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 TLoss: 2.1761 Acc: 0.2156 VLoss: 1.9443 Acc: 0.3282\n",
            "Epoch: 2 TLoss: 1.9981 Acc: 0.2866 VLoss: 1.7859 Acc: 0.4133\n",
            "Epoch: 3 TLoss: 1.8888 Acc: 0.3359 VLoss: 1.6970 Acc: 0.4490\n",
            "Epoch: 4 TLoss: 1.8371 Acc: 0.3673 VLoss: 1.6602 Acc: 0.4388\n",
            "Epoch: 5 TLoss: 1.8223 Acc: 0.3588 VLoss: 1.6851 Acc: 0.4558\n",
            "Epoch: 6 TLoss: 1.7340 Acc: 0.4056 VLoss: 1.5574 Acc: 0.4660\n",
            "Epoch: 7 TLoss: 1.6757 Acc: 0.4256 VLoss: 1.5345 Acc: 0.4694\n",
            "Epoch: 8 TLoss: 1.6027 Acc: 0.4507 VLoss: 1.5411 Acc: 0.4898\n",
            "Epoch: 9 TLoss: 1.5685 Acc: 0.4588 VLoss: 1.3462 Acc: 0.5476\n",
            "Epoch: 10 TLoss: 1.5086 Acc: 0.4962 VLoss: 1.3262 Acc: 0.5680\n",
            "Epoch: 11 TLoss: 1.4508 Acc: 0.5085 VLoss: 1.2503 Acc: 0.5765\n",
            "Epoch: 12 TLoss: 1.3971 Acc: 0.5361 VLoss: 1.2036 Acc: 0.6105\n",
            "Epoch: 13 TLoss: 1.3458 Acc: 0.5425 VLoss: 1.1614 Acc: 0.6139\n",
            "Epoch: 14 TLoss: 1.2805 Acc: 0.5821 VLoss: 1.2008 Acc: 0.6224\n",
            "Epoch: 15 TLoss: 1.2258 Acc: 0.5918 VLoss: 1.0723 Acc: 0.6361\n",
            "Epoch: 16 TLoss: 1.1674 Acc: 0.6186 VLoss: 1.1329 Acc: 0.6020\n",
            "Epoch: 17 TLoss: 1.0891 Acc: 0.6395 VLoss: 1.0528 Acc: 0.6548\n",
            "Epoch: 18 TLoss: 1.0447 Acc: 0.6607 VLoss: 1.0467 Acc: 0.6531\n",
            "Epoch: 19 TLoss: 1.0075 Acc: 0.6624 VLoss: 1.0055 Acc: 0.6616\n",
            "Epoch: 20 TLoss: 0.9043 Acc: 0.7088 VLoss: 0.8575 Acc: 0.6939\n",
            "Epoch: 21 TLoss: 0.8623 Acc: 0.7275 VLoss: 0.9153 Acc: 0.6854\n",
            "Epoch: 22 TLoss: 0.8017 Acc: 0.7385 VLoss: 0.9279 Acc: 0.6956\n",
            "Epoch: 23 TLoss: 0.7627 Acc: 0.7530 VLoss: 0.8731 Acc: 0.7143\n",
            "Epoch: 24 TLoss: 0.7032 Acc: 0.7768 VLoss: 0.8370 Acc: 0.7092\n",
            "Epoch: 25 TLoss: 0.6933 Acc: 0.7832 VLoss: 0.8000 Acc: 0.7432\n",
            "Epoch: 26 TLoss: 0.6107 Acc: 0.8125 VLoss: 0.7946 Acc: 0.7347\n",
            "Epoch: 27 TLoss: 0.5906 Acc: 0.8095 VLoss: 0.7752 Acc: 0.7517\n",
            "Epoch: 28 TLoss: 0.5696 Acc: 0.8197 VLoss: 0.7315 Acc: 0.7636\n",
            "Epoch: 29 TLoss: 0.5193 Acc: 0.8427 VLoss: 0.6548 Acc: 0.7959\n",
            "Epoch: 30 TLoss: 0.4738 Acc: 0.8571 VLoss: 0.6945 Acc: 0.7704\n",
            "Epoch: 31 TLoss: 0.4834 Acc: 0.8559 VLoss: 0.7181 Acc: 0.7789\n",
            "Epoch: 32 TLoss: 0.4108 Acc: 0.8707 VLoss: 0.6864 Acc: 0.7789\n",
            "Epoch: 33 TLoss: 0.4493 Acc: 0.8661 VLoss: 0.7778 Acc: 0.7517\n",
            "Epoch: 34 TLoss: 0.4058 Acc: 0.8801 VLoss: 0.5825 Acc: 0.8095\n",
            "Epoch: 35 TLoss: 0.3964 Acc: 0.8763 VLoss: 0.6796 Acc: 0.7925\n",
            "Epoch: 36 TLoss: 0.3331 Acc: 0.8997 VLoss: 0.6789 Acc: 0.7772\n",
            "Epoch: 37 TLoss: 0.3268 Acc: 0.8997 VLoss: 0.6402 Acc: 0.7993\n",
            "Epoch: 38 TLoss: 0.3456 Acc: 0.8941 VLoss: 0.6374 Acc: 0.7891\n",
            "Epoch: 39 TLoss: 0.3187 Acc: 0.9086 VLoss: 0.6520 Acc: 0.8112\n",
            "Epoch: 40 TLoss: 0.2601 Acc: 0.9286 VLoss: 0.6809 Acc: 0.8027\n",
            "Epoch: 41 TLoss: 0.2557 Acc: 0.9213 VLoss: 0.6787 Acc: 0.7772\n",
            "Epoch: 42 TLoss: 0.2905 Acc: 0.9094 VLoss: 0.7727 Acc: 0.7925\n",
            "Epoch: 43 TLoss: 0.2668 Acc: 0.9218 VLoss: 0.6246 Acc: 0.8010\n",
            "Epoch: 44 TLoss: 0.2709 Acc: 0.9167 VLoss: 0.6144 Acc: 0.8231\n",
            "Epoch: 45 TLoss: 0.2550 Acc: 0.9286 VLoss: 0.7812 Acc: 0.7619\n",
            "Epoch: 46 TLoss: 0.2291 Acc: 0.9294 VLoss: 0.5983 Acc: 0.8129\n",
            "Epoch: 47 TLoss: 0.2182 Acc: 0.9345 VLoss: 0.5267 Acc: 0.8367\n",
            "Epoch: 48 TLoss: 0.1971 Acc: 0.9366 VLoss: 0.5352 Acc: 0.8350\n",
            "Epoch: 49 TLoss: 0.2038 Acc: 0.9456 VLoss: 0.5823 Acc: 0.8180\n",
            "Epoch: 50 TLoss: 0.2236 Acc: 0.9396 VLoss: 0.5845 Acc: 0.8418\n",
            "Flax Nested UNet Time: 1074.74s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flax_nested_state, flax_nested_history = train_flax_model(FlaxNestedUNet, train_loader, val_loader, HYPERPARAMS, \"Flax Nested UNet\")\n",
        "save_model_params(flax_nested_state, \"flax_nested_params.eqx\")\n",
        "try:\n",
        "  del flax_nested_state\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()\n",
        "jax.clear_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy9pnWqmvZNO"
      },
      "source": [
        "tabulated results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Rbyw0q5TmrZm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL RESULTS COMPARISON (5 MODELS)\n",
            "================================================================================\n",
            "Model                     Val Accuracy    Train Time (s) \n",
            "Baseline CNN              0.6531 743.33\n",
            "Unidirectional Mamba      0.4796 9839.68\n",
            "Bidirectional ViM         0.5544 19071.34\n",
            "DylanMamba                0.5952 7010.43\n",
            "Flax UNet                 N/A             N/A            \n",
            "Flax NestedUNet           N/A             N/A            \n"
          ]
        }
      ],
      "source": [
        "def safe_print_result(model_name, history_var_name):\n",
        "  if history_var_name in globals():\n",
        "    history = globals()[history_var_name]\n",
        "    val_acc_key = 'val_accuracy' if 'val_accuracy' in history else 'val_acc'\n",
        "    print(f\"{model_name:<25} {history[val_acc_key][-1]:.4f} {history['time']:.2f}\")\n",
        "  else:\n",
        "    print(f\"{model_name:<25} {'N/A':<15} {'N/A':<15}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS COMPARISON (5 MODELS)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<25} {'Val Accuracy':<15} {'Train Time (s)':<15}\")\n",
        "safe_print_result('Baseline CNN', 'baseline_history')\n",
        "safe_print_result('Unidirectional Mamba', 'unimamba_history')\n",
        "safe_print_result('Bidirectional ViM', 'bimamba_history')\n",
        "safe_print_result('DylanMamba', 'dylan_history')\n",
        "safe_print_result('Flax UNet', 'flax_unet_history')\n",
        "safe_print_result('Flax NestedUNet', 'flax_nested_history')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn4eP-0ovaaR"
      },
      "source": [
        "plotted results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HZcN4Rk7vA6W"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib' has no attribute 'pyplot'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m histories = {\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mBaseline CNN\u001b[39m\u001b[33m'\u001b[39m: baseline_history,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mUnidirectional Mamba\u001b[39m\u001b[33m'\u001b[39m: unimamba_history,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m#'Flax NestedUNet': flax_nested_history\u001b[39;00m\n\u001b[32m      8\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m fig, axes = \u001b[43mmatplotlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpyplot\u001b[49m.subplots(\u001b[32m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(histories), figsize=(\u001b[32m20\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m     11\u001b[39m fig.suptitle(\u001b[33m'\u001b[39m\u001b[33mTraining Curves for Each Model\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (name, history) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(histories.items()):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jax_env/lib/python3.12/site-packages/matplotlib/_api/__init__.py:218\u001b[39m, in \u001b[36mcaching_module_getattr.<locals>.__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name].\u001b[34m__get__\u001b[39m(instance)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    219\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib' has no attribute 'pyplot'"
          ]
        }
      ],
      "source": [
        "histories = {\n",
        "    'Baseline CNN': baseline_history,\n",
        "    'Unidirectional Mamba': unimamba_history,\n",
        "    'Bidirectional ViM': bimamba_history,\n",
        "    'DylanMamba': dylan_history\n",
        "    #'Flax UNet': flax_unet_history,\n",
        "    #'Flax NestedUNet': flax_nested_history\n",
        "}\n",
        "\n",
        "fig, axes = matplotlib.pyplot.subplots(2, len(histories), figsize=(20, 10))\n",
        "fig.suptitle('Training Curves for Each Model')\n",
        "\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    if history is not None and 'train_loss' in history:\n",
        "        epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "        axes[0, i].plot(epochs, history['train_loss'], label='Train Loss')\n",
        "        axes[0, i].plot(epochs, history['val_loss'], label='Val Loss')\n",
        "        axes[0, i].set_title(f'{name} - Loss')\n",
        "        axes[0, i].set_xlabel('Epoch')\n",
        "        axes[0, i].set_ylabel('Loss')\n",
        "        axes[0, i].legend()\n",
        "\n",
        "        # Dynamically select accuracy keys\n",
        "        train_acc_key = 'train_accuracy' if 'train_accuracy' in history else 'train_acc'\n",
        "        val_acc_key = 'val_accuracy' if 'val_accuracy' in history else 'val_acc'\n",
        "        axes[1, i].plot(epochs, history[train_acc_key], label='Train Acc')\n",
        "        axes[1, i].plot(epochs, history[val_acc_key], label='Val Acc')\n",
        "        axes[1, i].set_title(f'{name} - Accuracy')\n",
        "        axes[1, i].set_xlabel('Epoch')\n",
        "        axes[1, i].set_ylabel('Accuracy')\n",
        "        axes[1, i].legend()\n",
        "    else:\n",
        "        axes[0, i].text(0.5, 0.5, f'{name}\\nNo data', ha='center', va='center', transform=axes[0, i].transAxes)\n",
        "        axes[1, i].text(0.5, 0.5, f'{name}\\nNo data', ha='center', va='center', transform=axes[1, i].transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McQQ1x7AkFe_"
      },
      "source": [
        "# **Why this project is so f\\*\\*king hard!!!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (JAX Env)",
      "language": "python",
      "name": "jax_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
